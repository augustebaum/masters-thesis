\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}

\begin{document}

\chapter{Methods}
\label{ch:methods}

We now present our work in more detail.
First, we discuss the specific problems we addressed and offer our proposed solutions along with theoretical justifications.
Then, we introduce the metrics and datasets that will be used in our experiments.

\section{Problem 1: Multi-class extension}

\subsection{Problem statement}

The authors of \ls{} solve the following problem \cite{cohenGifsplanation2022}:

Let $f: \R^\inputdim \to [0, 1]$ be a binary classifier such that $f(x)$ outputs the predicted probability of $x$ being in class 1.
Let $x$ be a point that is predicted to be of class 0, \ie{} such that $f(x) < \frac{1}{2}$.
Find a path starting at (or around) $x$ and ending at a point $\CF{x}$ for which $f(\CF{x}) > \frac{1}{2}$ such that the path is sufficiently simple and easily comprehensible by a human.

They propose building a latent representation $\autoencoder = (\enc, \dec)$, letting $z = \enc(x)$ and computing the gradient of the classifier in that latent space:
$\nabla_z f(\dec(z))$.
Then they can recover a potential counterfactual by shifting the latent along the gradient: $\CF{x} = \dec(z + \lambda \nabla_z f(\dec(z)))$ where $\lambda$ is some positive real number.
Since the gradient gives the direction of steepest ascent, going in this direction is the best strategy to increase $f(\dec(z))$ and therefore obtain a valid $\CF{x}$.

In the case of a binary classifier that outputs a scalar, this might be sufficient. However, in multi-class problems where $f$ outputs a vector in $\R^\outputdim$ there are several possible choices of functions of which we could compute the gradient.
To see this, we reformulate the latent shift strategy with a generic loss that accounts for validity, denoted by $\lossval$:

Let $f$ be the classifier of interest, which outputs logits in $\R^\outputdim$ rather than a probability mass.
Given $x \in \inputspace$ and an autoencoder $\autoencoder = (\enc, \dec)$, a counterfactual path can be generated by computing the gradient of the validity loss $\lossval$ and perturbing $z = \enc(x)$ along the opposite of the gradient (that is, in the direction of steepest \emph{descent}) so as to minimize $\lossval$:
$\CF{x} = \dec(z - \lambda \nabla_z \lossval(z))$.

In vanilla \ls{}, the validity loss would then be $\lossval(z) = - \sigma(f(\dec(z)))$ where $\sigma$ denotes the sigmoid function, or any other increasing differentiable function that outputs a number in $[0, 1]$.

Hence, our problem is as follows:
Find a validity loss $\lossval$ that, for a given $x \in \inputspace$, gives rise to a class-changing path in input space when used in a gradient-based path generation method such as the one described above.

\subsection{Proposed solutions}

Note that this extension to the multi-class setting means that we have a choice to specify a given target class: we might also prefer to simply change the class, without preference of target class. In our analysis we focus on the case where a target class is specified.

In order to mimic the solution discussed in \cite{cohenGifsplanation2022}, we could use
\begin{equation}
    \lossval(z) = -\softmax(f(\dec(z)))_\target,
\end{equation}
\ie{} the component of the output probability mass corresponding to class $\target$.
As before, the codomain is $[0, 1]$, and the loss corresponds directly to what we wish to achieve.


% However, because the codomain is small, the gradients computed from this loss could potentially be very small in magnitude and thus unreliable.
% By contrast, a typical loss function used in classification tasks (such as the cross-entropy) often includes a logarithm so that the loss can change more rapidly which promotes larger gradients. \citenote{}
% Hence, we use the cross-entropy instead: $$\lossval = \ce(f(\dec(z)), y_\target).$$
% We call this validity loss \method{TargetLoss}.

% There are other possible variants. For example, one might wish to specify that, if not able to reach the target class, the path should at least get away from the source class. This can be included as follows:
% $$\lossval = \ce(f(\dec(z)), y_\target) - \ce(f(\dec(z)), y_\source).$$
% We call this variant \method{BinaryStretchLoss}.

% Taking this idea further, one might wish that the path should get away from all classes that are not the target, which we could model as follows:
% $$\lossval = \ce(f(\dec(z)), y_\target) - \sum_{c \neq \target} \ce(f(\dec(z)), y_c).$$
% We call this \method{StretchLoss}. 

We could stop there, but in fact other possibilities arise because of the multi-class assumption.

Indeed, in the binary classification case, reaching towards the target class is exactly the same as moving away from the source class, since $p_\source = 1 - p_\target$.
By contrast, in the multi-class setting, different priorities need to be distinguished on a case-by-case basis: is it more important to reach the target (at the risk of potentially staying in the source class) or to get away from the source (at the risk of not reaching the target)?
The natural extension we presented is favoring the first option, but perhaps some variation of it could be preferable.

Thus, we introduce the following alternative loss, with regularization $\lambda \in [0, 1]$:
\begin{equation}
    \lossval(z) = -\softmax(f(\dec(z)))_\target
+ \lambda \softmax(f(\dec(z)))_\source
\end{equation}
which can be interpreted as adding the constraint of going away from the source as well as going towards the target.

To validate our intuition, we can compute the gradient of the validity loss:
\begin{align}
    \grad{\lossval}{z}
    =  \grad{}{z} \left( - \softmax(f(\dec(z)))_\target + \softmax(f(\dec(z)))_\source \right)
\end{align}

Because $z$ will only ever appear within $f(\dec(z))$, let $u = f(\dec(z))$; by the chain rule, we have $\grad{\lossval}{z} = \grad{\lossval}{u} \grad{u}{z}$.

$\grad{u}{z}$ does not change with the loss so we focus on $\grad{\lossval}{u}$:
\begin{align*}
    \grad{\lossval}{u}
    =  \grad{}{u} \left( - \softmax(u)_\target + \softmax(u)_\source \right)
\end{align*}

Let us compute the gradient of $\softmax$ in general:
\begin{align*}
    \grad{\softmax(u)_i}{u_j}
     & = \grad{}{u_j} \frac{\eu{i}}{\seu}                                                 \\
     & = \frac{\grad{\eu{i}}{u_j} \seu - \eu{i} \grad{}{u_j} \seu}{\left( \seu \right)^2} \\
& = \frac{\dij \eu{i} \seu - \eu{i} \eu{j}}{\left(\seu \right)^{2}}           \\
& = \dij \frac{\eu{i}}{\seu} - \frac{\eu{i}}{\seu} \frac{\eu{j}}{\seu}        \\
& = \softmax(u)_i ( \dij  -  \softmax(u)_j)
\end{align*}
To lighten the notation we define $p_i = \softmax(u)_i$.
With this, the gradient of the loss is now written:
\begin{align*}
    \grad{\lossval}{u_j}
    =  - & \grad{p_\target }{u_j} + \grad{p_\source}{u_j}  \\
    =  - & p_\target (\djt - p_j) + p_\source (\djs - p_j)
\end{align*}

What can we learn from this?
The validity loss should fulfill some basic requirements, which we list here:
\begin{itemize}
    \item When $p_\target$ is 1, we expect the gradient to be close to 0; indeed, if the target is reached, there is no reason to keep perturbing $x$.
    \item When $p_\target$ is 0, we expect the gradient to be lower in the component corresponding to $\target$, so that the opposite of the gradient is higher in that component, which indicates the path is pushed towards $\target$.
\end{itemize}

Let us now examine whether these requirements are satisfied:
\begin{itemize}
    \item When $p_\target$ is 1, this means $p_j = \djt$, so we have for all $j$:
          \begin{equation*}
              \grad{\lossval}{u_j} = -1 \times (\djt -\djt) + 0 \times (\djs - p_j) = 0
          \end{equation*}
    \item When $p_\target$ is 0, we get:
          \begin{align*}
              \grad{\lossval}{u_j}
               & = -0 + p_\source (\djs - p_j)                        \\
               & = \begin{cases}
                       0                         & \text{if } j = \target \\
                       p_\source (1 - p_\source) & \text{if } j = \source \\
                       - p_\source p_j           & \text{otherwise}
                   \end{cases}
          \end{align*}
          The components for the classes that are not $\source$ or $\target$ are negative, which means these classes are favored compared to $\target$ which is counter to our intuition.
\end{itemize}
Compare this with our naive extension:
for $\lossval(u) = -p_\target$, the gradient is
$\grad{\lossval}{u_j} =  -  p_\target (\djt - p_j)$
so when
$p_\target = 1$, $\grad{\lossval}{u_j} = -1 \times (\djt - \djt) = 0$ as required,
and when
$p_\target = 0$, $\grad{\lossval}{u_j} = 0$ so the loss does not increase $u_\target$ at all (or any other class), which is not optimal.

\subsection{Log-probability-based losses}

Setting the probability as an objective goes counter to the usual loss functions for training classifiers; a more common choice is the cross-entropy $\ce$.
Indeed, the cross-entropy represents the information that is shared between two distributions, which is a more natural metric than a conventional distance such as the $L_2$ norm \cite{murphyMachine2012}.
The cross-entropy involves the logarithm of the predicted probabilities, so we can easily modify our previous attempts to get:
\begin{align*}
    \LogPrbTarget{}:        & \quad    \lossval(u) = - \log p_\target                              \\
    \LogPrbSource{\lambda}: & \quad     \lossval(u) = - \log p_\target + \lambda \log p_\source    \\
    \LogPrbOthers{\lambda}: & \quad    \lossval(u) = - \log p_\target + \lambda \mysum{c} \log p_c
\end{align*}
where $\lambda \in [0, 1]$ is a regularization parameter.
$\LogPrbOthers{}$ encodes the objective to not only increase the probability of $\target$ but also to decrease all the others.
This is important because in \ls{} the objective of increasing $p_\target$ is equivalent to decreasing $\mysum{c} p_c$, since in binary classification this is just $p_\source$; however, in multi-class problems they are distinct quantities.

In fact, with this naming scheme we can also give names to the previous losses:
\begin{align*}
    \PrbTarget{}:        & \quad    \lossval(u) = - p_\target                         \\
    \PrbSource{\lambda}: & \quad     \lossval(u) = - p_\target + \lambda p_\source    \\
    \PrbOthers{\lambda}: & \quad    \lossval(u) = - p_\target + \lambda \mysum{c} p_c
\end{align*}
But notice
\begin{align*}
    \PrbOthers{\lambda}(u)
     & = - p_\target + \lambda (1 - p_\target) \\
     & = -(\lambda + 1)p_\target + \lambda
\end{align*}
which resembles $\PrbTarget{}$, so in practice $\PrbOthers{}$ is not used.

Let us compute the gradients of our log-probability losses to get a better understanding of their behavior.

We have
\begin{align*}
    \grad{\log p_i}{u_j}
     & = \frac{\grad{p_i}{u_j} }{p_i}  \\
     & = \frac{p_i (\dij - p_j) }{p_i} \\
     & = \dij - p_j
\end{align*}
Hence
\begin{align*}
    \grad{\LogPrbSource{\lambda}}{u_j}
     & = - \grad{\log p_\target}{u_j} + \lambda \grad{\log p_\source}{u_j} \\
     & = - (\djt - p_j) + \lambda (\djs - p_j)                             \\
     & = - \djt + p_j + \lambda \djs - \lambda p_j                         \\
     & = (1 - \lambda) p_j + \lambda \djs - \djt
\end{align*}

We check our intuition:
\begin{itemize}
    \item For $p_\target = 1$ this gives us
          \begin{align*}
              \grad{\LogPrbSource{\lambda}}{u_j}
               & = (1 - \lambda) \djt + \lambda \djs - \djt \\
               & = \lambda(\djs - \djt)                     \\
               & = \begin{cases}
                       -\lambda & \text{if } j = \target \\
                       \lambda  & \text{if } j = \source \\
                       0        & \text{otherwise}
                   \end{cases}
          \end{align*}
          so the gradient pushes the logit of class $\target$ higher and that for class $\source$ lower.
          When $\lambda = 0$, which corresponds to $\LogPrbTarget$, the gradient is zero which is also acceptable intuitively.

    \item For $p_\target = 0$ we get
          \begin{align*}
              \grad{\LogPrbSource{\lambda}}{u_j}
               & = (1 - \lambda) p_j + \lambda \djs - \djt                 \\
               & = \begin{cases}
                       0 + 0 - 1 = -1                 & \text{if } j = \target \\
                       (1-\lambda)p_\source + \lambda & \text{if } j = \source \\
                       (1 - \lambda) p_j              & \text{otherwise}
                   \end{cases}
          \end{align*}
          So when $j \neq \target$ the gradient is larger than 0; again, class $\target$ dominates.
\end{itemize}

For $\LogPrbOthers{}$ we have:
\begin{align*}
    \grad{\LogPrbOthers{\lambda}}{u_j}
     & = - \grad{\log p_\target}{u_j} + \lambda \mysum{c}\grad{\log p_c}{u_j}          \\
     & = - (\djt - p_j) + \lambda \mysum{c} (\delta_{cj} - p_j)                        \\
     & = - \djt + p_j + \lambda (\mysum{c} \delta_{cj}) - \lambda \mysum{c} p_j        \\
     & = - \djt + p_j + \lambda (\mysum{c} \delta_{cj}) - \lambda p_j \mysum{c} 1      \\
     & = - \djt + p_j + \lambda (\mysum{c} \delta_{cj}) - \lambda p_j (\outputdim - 1) \\
     & = - \djt + p_j (1 - \lambda (\outputdim - 1)) + \lambda (\mysum{c} \delta_{cj}) \\
     & = \begin{cases}
             - 1 + p_\target (1 - \lambda (\outputdim - 1))   & \text{if } j = \target \\
             0 + p_j (1 - \lambda (\outputdim - 1)) + \lambda & \text{otherwise}
         \end{cases}     \\
     & = - \djt + p_j (1 - \lambda (\outputdim - 1)) + \lambda (1 - \djt)              \\
     & = p_j (1 - \lambda (\outputdim - 1)) + \lambda (1 - \djt) - \djt
\end{align*}

We check our intuition:
\begin{itemize}
    \item For $p_\target = 1$ this gives us
          \begin{align*}
              \grad{\LogPrbOthers{\lambda}}{u_j}
               & = \djt (1 - \lambda (\outputdim - 1)) + \lambda (1 - \djt) - \djt      \\
               & = \djt - \djt \lambda (\outputdim - 1) + \lambda - \lambda \djt - \djt \\
               & = - \djt \lambda (\outputdim - 1) + \lambda - \lambda \djt             \\
               & = - \lambda (\djt (\outputdim - 1) - 1 + \djt)                         \\
               & = - \lambda (\outputdim \djt - 1)                                      \\
               & = \begin{cases}
                       - (\outputdim - 1) \lambda & \text{if } j = \target \\
                       \lambda                    & \text{otherwise}
                   \end{cases}
          \end{align*}
          so the gradient pushes the logit of class $\target$ higher than that for other classes

    \item For $p_\target = 0$ we get
          \begin{align*}
              \grad{\LogPrbOthers{\lambda}}{u_j}
               & = p_j (1 - \lambda (\outputdim - 1)) + \lambda (1 - \djt) - \djt        \\
               & = \begin{cases}
                       -1                                           & \text{if } j = \target \\
                       p_j(1 - \lambda ( \outputdim -1 )) + \lambda & \text{otherwise}
                   \end{cases}
          \end{align*}
          Can a component for class $j$ be more negative than -1? Yes: for example, if $\lambda = 1$ and some $p_j = 1$ then we get $(1 - (\outputdim -1)) + 1 = 3 - \outputdim$ so for $\outputdim > 4$ logit $j$ will be pushed further up than $\target$.
\end{itemize}


\subsubsection{Logit-based losses}

Our previous loss functions are somewhat difficult to control because of the interactions between the different classes:
when we state our requirement to increase $p_\target$, this can be understood either as increasing $u_\target = f(\dec(z))_\target$, or as decreasing the other $u_c$.
For this reason, we revisit the previous attempts but optimizing for the logits directly.

Hence, we define the following losses, with regularization parameter $\lambda \in [0, 1]$:
\begin{align*}
    \LogitTarget{}:        & \quad    \lossval(u) = - u_\target                         \\
    \LogitSource{\lambda}: & \quad     \lossval(u) = - u_\target + \lambda u_\source    \\
    \LogitOthers{\lambda}: & \quad    \lossval(u) = - u_\target + \lambda \mysum{c} u_c
\end{align*}

Here the gradients are easy to compute since the loss is linear in the logits:
\begin{align*}
\grad{\LogitTarget{}}{u_j} &= - \djt                         \\
\grad{\LogitSource{\lambda}}{u_j} &= - \djt + \lambda \djs                        \\
\grad{\LogitOthers{\lambda}}{u_j} &= - \djt + \lambda (1-\djt) = -(\lambda + 1)\djt + \lambda
\end{align*}




\section{Problem 2: Path regularization}

\subsection{Problem statement}

A typical requirement on a counterfactual $\CF{x}$ for an input $x$ might be for $\norm{\CF{x} - x}$ to be small, where $\norm{\cdot}$ could be the $L_1$ norm for example \note{cite}.
While this makes sense for methods that produce a discrete set of counterfactuals, we focus on methods that produce \emph{continuous explanation paths}, hence we wish to ensure these requirements are satisfied \emph{along the path}.
In the case of distance, we might wish that the endpoint of the path $\CF{x} = \dec(z_T)$ be close to the starting point, but also that \emph{on the whole} the path $(z_t)_{t=1}^T$ stay as close as possible to the starting point.

In the case of gradient-descent optimization paths, this can be addressed by placing these constraints in the loss function during the computation of the next step: this is the case in \revise{}, where the loss function includes the $L_1$ distance \cite{joshiRealistic2019}:
\begin{equation*}
    \loss_{CF}(z_t) = \lossval(z_t) + \lambda_\text{distance} \norm{\dec(z_t) - x}_1.
\end{equation*}
However, for \ls{} this solution is not appropriate because the gradient is only computed once, for $z = \enc(x)$: even though the first step might be in the right direction, overall the whole path could still go in the wrong direction because the information carried by the gradient is less relevant the further away we go from the input.
In other words, this solution would amount to optimizing not over the path, but over the first step of the path.

Hence, our second problem statement is the following: in the case of gradient-descent methods it makes sense to use losses that take a $z_t$ as input, but how can we constrain a whole path $(z_t)_{t=1}^T$?

\subsection{Proposed solution}

Since we can extend many of the point-wise measures to whole paths in a natural way, \eg{} with the mean or the area-under-curve, we propose using this as a loss on the whole path, which could be used to \emph{train the autoencoder} in addition to its usual loss function.
This added component to the loss can take various forms, and in general we call it \emph{path regularization}. The full process is described with pseudocode in \autoref{algo:pathreg}.
Note that in NF-based autoencoders, the decoder is the inverse of the encoder and thus does not need to be trained like it would in a VAE; this is why we show explicitly only the trainable parameters of $\enc$ (denoted by $\phi$).

\begin{algorithm}
\caption{Learning a normalizing flow latent space by SGD with path regularization}
\label{algo:pathreg}
\KwData{Classifier $f, \autoencoder\ (\enc_\phi, \dec),$ (training) data, CF loss $\cfloss$, path loss $\pathloss$, path loss regularization coefficient $\lambda_\apath$}
\KwOut{$\phi^*$ that minimizes the $\nll$ and the path loss}
\While{not converged}{
$x \gets \mathtt{get\_input}(\text{data})$ \\
$z \gets \enc_\phi(x)$ \\
$\source \gets f(x)$ \\
$\target \gets \mathtt{random\_target}(x)$ \tcp*{such that $\target \neq \source$}
$\apath \gets \mathtt{generate\_path}_{\cfloss}(z, \source, \target)$ \tcp*{$\apath = (\CF{x}_t)_{t=1}^T$}
$\phi \gets \phi - \nabla_\phi {(\nll(\phi) + \lambda_\apath \pathloss(\apath))}$ \\
}
\end{algorithm}

Since the one obvious constraint on paths is that they should be valid, one way to include this in $\pathloss$ could be to add a count of the number of valid paths in the batch.
Note that class information is indirectly given during the training of the autoencoder, so the autoencoder is dependent on the classification task.
By contrast, in vanilla \ls{} the authors make a conscious decision to keep the autoencoder entirely separate from the classifier.
Their justification is that this allows for reusing the autoencoder on classifiers that have learned different tasks but that act on the same input data; for example, different classifiers each trained to detect a particular lung disease.
This is a perfectly sound argument, but in the CFX generation literature class-aware generative models tend to be the norm because class information is very valuable when the objective is to change the class.

\section{Robustness of path regularization}
\label{methods:robustness}

\subsection{Problem statement}

The path regularization algorithm admits a general loss $\pathloss$ which is problem-dependent.
Yet, this raises a question: can the path regularization algorithm accommodate for any path loss, however extraneous?
If so, this procedure could potentially be manipulated to produce adversarial paths that do not reflect the true behavior of the explained model.

\subsection{Proposed experiment}

To probe the possibility of abuse of the path regularization procedure, we formulate an adversarial loss that promotes paths going through a specific point $\xgoal$:
\begin{align}
    \pathloss(\xgoal, (z_t)_{t=1}^T) = \min_{t \in \{1, \ldots, T\}} \{ \norm{\dec(z_t) - \xgoal}_2 \}
\end{align}

\section{Metrics}

\subsection{Validity}

Validity is the one requirement where the metric of interest is unambiguous: either a path is valid or it is not.
Hence, for testing a path generation method on some test set, we measure the validity rate, which is the proportion of valid paths.

\subsection{Computational performance}

Ensuring that CFs generation is fast is an important concern for stakeholders, especially since one might need to generate paths for many points of interest.
Hence, we measure the time taken for one step of one path.
This is because depending on the number of steps chosen by the user, a path can take longer to generate, and methods such as \ls{} can take advantage of GPU optimizations to compute many paths at once.

\subsection{Realism}


% \paragraph{NF negative log-likelihood}

% The first method is utilizing a normalizing flow.

Following the reasoning presented in \cite{vanlooverenInterpretable2021}, we measure realism using a generative model.
However, while \citeauthor{vanlooverenInterpretable2021} use a VAE and thus estimate realism using the reconstruction error, we can compute the likelihood directly.
Indeed, normalizing flows are typically trained to minimize the negative log-likelihood (or NLL for short ) $\nll = - \log P(x)$, so we can compute it easily.
$P(x)$ is in $[0, 1]$ so the $\nll$ is in $\R_+$ and the closer it is to 0, the more realistic the point is deemed to be.


% \paragraph{Local Outlier Factor}

% The other metric we consider is the local outlier factor (or LOF for short) \cite{breunigLOF2000}, for which an implementation is provided by the \sklearn{} package\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html}}.
% The LOF score is derived from comparing the density around the point of interest to the density of its $k$ nearest neighbors: if the point has a low density compared to its neighbors then it is more likely to be an outlier.
% In our experiments we take the $L_2$ norm as the distance and $k = 20$ neighbors as these are the default values set in the \sklearn{} implementation.
% The score is in $]-\infty, 1]$; the higher the score, the more the point is considered to be an inlier and a negative score is an indication that the point is an outlier.

\subsection{Extending metrics to a path}

Many metrics take a point $x' \in \inputspace$ as input and return a score, but we wish to score a whole path $(x_t)_{t=1}^T$.
Hence, given a metric, we aggregate over the path (that is, from its starting point to the first step at which $\target$ is reached).

For this, we first produce paths in the form of a 3-dimensional tensor in $\R^{\nbsteps \times \batchsize \times \inputdim}$.
We compute the model classes resulting in a 2-dimensional tensor in $\setclasses^{\nbsteps \times \batchsize}$.
Based on this and our target classes in $\R^\batchsize$, we construct a mask $\pathmask \in \{0, 1\}^{\nbsteps \times \batchsize}$ that highlights the valid paths:
if path $j$ is not valid, then
$\forall i \in \{1, \ldots, \nbsteps\} \pathmask_{ij} = 0$, and if path $j$ is valid then $\pathmask_{ij} = 1$ up to and \emph{not including} the first index where $\target$ is reached, and 0 after that.

Then we can take the element-wise product of, \eg{} the $\nll$ computed for all points of all paths, to mask the unwanted points.

Note that if no paths are valid, the resulting product will be equal to zero, which can be misleading: $\nll = 0$ is a perfect score.
For this reason we take care to always report the validity rate alongside any metric computed this way.

\section{Datasets}
\label{sec:datasets}

In our experiments we use one custom synthetic dataset generated in a parametrizable way, as well as some real datasets.

We always take out the categorical features and we standardize each feature.

\subsection{\CakeOnSea}

Our analyses were complicated by the impossibility of visualizing counterfactuals for tabular data in general.
Thus, in order to start approaching the problem intuitively, we developed a synthetic dataset with no more than two features that have an influence on the response.
The two features, denoted by $x_0$ and $x_1$, range from 0 to 50, except for the so-called ``dead zone'' which is $[25, 35]^2$, in which there are no points at all.
There are three classes, and the class decision rule is as follows:
\begin{align*}
    (x_0, x_1) \in [35, 45]^2 \implies & y = 2 \\
    x_1 < 25                  \implies & y = 0 \\
    (x_0, x_1) \in \text{dead zone} \lor (x_0, x_1) \notin [0, 50]^2  \implies & y\ \text{unknown} \\
    \text{otherwise} \qquad            & y = 1
\end{align*}
Note that we explicitly refrain from assigning a class if $x = (x_0, x_1)$ is in the dead zone, to simulate a situation with completely unrealistic points.
The decision rule can be visualized in \autoref{fig:cake_on_sea}.

To generate the dataset, we randomly sample points in the $[0, 50]^2$ and then take out points from the dead zone. By default, the sampling distribution is a uniform $\mathcal{U}([0, 50]^2)$.

\begin{figure}[h]
    \centering
    \input{Figures/cake_on_sea}
\caption{Decision rule for the \CakeOnSea dataset.}
    \label{fig:cake_on_sea}
\end{figure}

\subsection{\ForestCover}

This dataset is taken from the UCI Machine Learning repository \cite{duaUCI2019} based on work in \cite{blackardComparative1999}. It was found at this URL: \url{https://archive.ics.uci.edu/ml/datasets/Covertype}.
It contains data on 581012 trees such as their elevation, their horizontal distance to the nearest surface water features and the type of soil they are in, as well as their species, among seven tree species. There is a total of 13 recorded variables, including the tree species. Of the 12 variables used for prediction, two are categorical, and encoded as one-hot columns.
Because our method does not trivially extend to categorical features yet, we remove the one-hot columns; we thus have $D = 10$.

\subsection{\WineQuality}

The \WineQuality{} dataset originates from \cite{cortezModeling2009} and can be found on the UCI repository at this link: \url{https://archive.ics.uci.edu/ml/datasets/Wine+Quality}.
It consists of data on around 6498 Portuguese ``Vinho Verde'' wines, such as their pH, their density and their alcohol percentage, as well as a quality rating from 0 to 10.
There are 13 variables including the quality rating column; except for the quality and whether they are red or white, all of them are numerical.
As before, we remove the red/white information.

Because the wine quality is an ordinal variable (categorical but ordered), it might not be appropriate to run a classifier on the dataset directly.
To mitigate this we group quality ratings together into 3 classes.
In fact, there are no wines with a quality rating of 0, 1, 2 or 10, and 43\% of the rows have a rating of 6 out of 10.
Hence, for our classification task we create the following class mapping:
\begin{itemize}
    \item class 0 corresponds to a quality rating of 5 or lower,
    \item class 1 to a rating of 6,
    \item class 2 to a rating of 7 or higher.
\end{itemize}
With this mapping, around 33\% of data are in class 0, 43\% in class 1 and 18\% in class 2.

\subsection{\OnlineNewsPopularity}

This dataset \cite{fernandesProactive2015} contains data on 39644 news posts recovered from the Mashable website, such as the number of HTML tags of a certain type, the time of publishing, and also machine-learning-powered features such as the subjectivity and polarity of the language used, and how well the post fits within a given topic as determined by LDA.
As before, we remove the categorical columns, leaving us with 46 columns including the target variable.

The target variable is the number of shares of a given article, which is an integer; in order to make it usable for classification tasks, we create the following class mapping:
\begin{itemize}
    \item Articles with a number of shares below the median (\ie{} in the worst 50\%) are considered to be in class 0,
    \item Articles between the 50\% and the 75\% percentiles are considered class 1,
    \item Articles between the 75\% and 95\% percentiles are considered class 2,
\item Articles above the 95\% percentile (\ie{} in the best 5\%) are considered class 3.
\end{itemize}
Thus, class 0 contains 50\% of the data, class 1 contains 25\%, class 2 contains 20\% and class 3 contains 5\%.

\subsection{Summary of datasets}

All datasets are divided into training, validation, and test sets containing 60\%, 20\%, and 20\% of the data respectively.

In \autoref{tab:datasets} we summarize the relevant information about our datasets and settings when using them for training.

\begin{table}[h!]
    \centering
    \caption{Summary of our dataset settings}
    \label{tab:datasets}
\begin{tabular}{lrrrrrrr}
        \toprule
Name          & $\inputdim$ & $\outputdim$ & $\card{\trainset}$     & $\card{\valset}$     & $\card{\testset}$    & $
    \card{\trainset} / \inputdim$ & $\batchsize$ \\
        \midrule
\CakeOnSea            & 2           & 3            & 57607                  & 19202                & 19202        & 28803         & ??           \\
\ForestCover          & 10          & 7            & 348608                 & 116202               & 116202       & 34861        & ??           \\
\WineQuality          & 11          & 3            & 3900                   & 1299                 & 1299          & 355        & ??           \\
\OnlineNewsPopularity & 45          & 4            & 23788                  & 7928                 & 7928          & 529        & ??           \\
        \bottomrule
    \end{tabular}
\end{table}


\end{document}
