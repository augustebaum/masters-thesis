\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}

\begin{document}

\chapter{Methods}
\label{ch:methods}

\section{Problem 1: Multi-class extension}

\subsection{Problem statement}

The authors of \ls{} solve the following problem:

Let $f: \R^\inputdim \to [0, 1]$ be a binary classifier such that $f(x)$ outputs the predicted probability of $x$ being in class 1.
Let $x$ be a point that is predicted to be of class 0, \ie{} such that $f(x) < \frac{1}{2}$.
Find the trajectory of quickest class change for $x$; that is, a path starting at (or around) $x$ and ending at a point $\CF{x}$ for which $f(\CF{x}) > \frac{1}{2}$ such that the path is sufficiently short.

They propose building a latent representation $\autoencoder = (\enc, \dec)$, letting $z = \enc(x)$ and computing the gradient of the classifier in that latent space:
$\nabla_z f(\dec(z))$.

In the case of a binary classifier that outputs just one scalar, this might be sufficient. However, in multi-class problems where $f$ outputs a vector in $\R^\outputdim$ there are several possible choices for the gradient computation.
We formulate the problem as follows:

Let $f: \R^\inputdim \to \R^\outputdim$ be a multi-class classifier such that $f(x)$ outputs logits which can be transformed into a probability mass for the predicted class of $x$, \eg{} by a $\mathrm{softmax}$.
Consider a latent representation $\autoencoder = (\enc, \dec)$ of the input space. 
Let $z = \enc(x)$.
Find a validity loss $\lossval$ such that
$\nabla_z \lossval$ gives rise to a class-changing path in input space.

\subsection{Proposed solutions}

Note that this extension to the multi-class setting means that we have a choice to specify a given target class: we might also prefer to simply change the class, without preference of target class. In our analysis we focus on the case where a target class is specified.

At first glance, it seems that $\lossval = -f(\dec(z))_\target$ (the component of $f(\dec(z))$ corresponding to class $\target$) could be a satisfactory generalization: minimizing this loss is equivalent to maximizing $f(\dec(z))_\target$.
However, one issue with this choice of $\lossval$ is its output domain: $[-1, 0]$, which is small. This means that the gradients computed from this loss could potentially be very small in magnitude and thus unreliable.
By contrast, a typical loss function used in classification tasks (such as the cross-entropy) often includes a logarithm so that the loss can change more rapidly which promotes larger gradients. \citenote{}
Hence, we use the cross-entropy instead: $$\lossval = -\ce(f(\dec(z)), y_\target).$$
We call this validity loss \method{TargetLoss}.

There are other possible variants. For example, one might wish to specify that, if not able to get near the target class, the path should at least get away from the source class. This can be included as follows:
$$\lossval = -\ce(f(\dec(z)), y_\target) + \ce(f(\dec(z)), y_\source).$$
We call this variant \method{BinaryStretchLoss}.

Taking this idea further, one might wish that the path should get away from all classes that are not the target, which we could model as follows:
$$\lossval = -\ce(f(\dec(z)), y_\target) + \sum_{c \neq \target} \ce(f(\dec(z)), y_c).$$
We call this \method{StretchLoss}. 

% ---

\section{Problem 2: Path regularization}

\subsection{Problem statement}

A typical requirement on a counterfactual $\CF{x}$ for an input $x$ might be $\norm{\CF{x} - x}$ is small, where $\norm{\cdot}$ is the $L_1$ norm for example.
While this makes sense for methods that produce a discrete set of counterfactuals, we focus on methods that produce continuous explanation paths, hence we wish to extend these requirements to suit this goal.
For instance, in the case of distance, we might wish that the endpoint of the path be close to the starting point, but also that \emph{on the whole} the path stay as close as possible to the starting point.

In the case of gradient-descent optimization paths, this can be addressed by placing these constraints in the loss function during the computation of the next step: this is the case in \revise{}, where the loss function includes the $L_1$ distance \cite{joshiRealistic2019}.

However, for \ls{} this option is not optimal because the gradient is only computed once: even though the first step might be in the right direction, it might be that the whole path could go in the wrong direction, because the information carried by the gradient is less relevant the further away we go from the input.
In other words, we would not be optimizing over the path, but only over the first step of the path.

In the case of gradient-descent methods it makes sense to use losses that take a $z_t$ as input, but how could we constrain a whole path $(z_t)_{t=1}^T$?

\subsection{Proposed solution}

Since we can extend many of the point-wise measures to whole paths in a natural way, \eg{} with the mean or the area-under-curve, we propose using this a loss on the whole path, which could be used to train the autoencoder in addition to its usual loss function.
This added component to the loss can take various forms, and in general we call it \emph{path regularization}. The full process is described with pseudocode in \autoref{algo:pathreg}.
Note that in the case of normalizing flow-based representations, the decoder is the inverse of the encoder and thus does not need to be trained the way it would in a VAE. This is why we only show explicitly the parameters of $\enc$ (denoted by $\phi$).

\begin{algorithm}
\caption{Learning a normalizing flow latent space by SGD with path regularization}
\label{algo:pathreg}
\KwData{$f, (\enc_\phi, \dec),$ data, path loss $\loss$}
\KwOut{$\phi^*$ that minimizes the $\nll$ and the path loss}
\While{not converged}{
    $x \gets \mathtt{get\_input}(\text{data})$ \\
    $z \gets \enc_\phi(x)$ \\
    $\source \gets f(x)$ \\
    $\target \gets \mathtt{random\_target}(x)$
        \tcp*{such that $\target \neq \source$}
    $\apath \gets \mathtt{generate\_path}(z, \source, \target)$
        \tcp*{$\apath = (\CF{x}_t)_{t=1}^T$}
    $\phi \gets \phi - \nabla_\phi {(\nll(\phi) + \loss(\apath))}$ \\
}
\end{algorithm}

\section{Metrics}

\section{Datasets}

CakeOnSea

ForestCover: took out categorical columns.
Over 500k rows. 7 categories.

WineQuality: Around 6k rows. Response is quality from 0 to 10, but in fact there are no rows with quality 0, 1, 2 or 10, and 43\% of the rows have a rating of 6.
Hence we make the following mapping: class 0 corresponds to a quality rating of 5 or lower, class 1 to a rating of 6, and class 2 to a rating of 7 or higher.
With this mapping, around 33\% of data are in class 0, 43\% in class 1 and 18\% in class 2.

\end{document}