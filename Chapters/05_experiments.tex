\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}

\begin{document}

\chapter{Experiments}

We now present our experiments to address the problems stated in \autoref{ch:methods}.

First we give details on settings common to all experiments, next we present our metrics of choice
and then we delve into the experiments themselves, presenting the experimental setup, the results and
discussion for each of them in turn.

\section{Models}

All models are trained using the Adam algorithm \cite{kingmaAdam2014} with a learning rate of $1 \times 10^{-3}$.
We let the models train for an undetermined number of epochs, performing early stopping based on the validation loss with a patience parameter of 3.

The models are implemented in \texttt{pytorch} and trained on a machine with the following characteristics:
\begin{itemize}
    \item CPU: Intel Core i7-7700K CPU @ 4.20GHz
    \item RAM: 32 GB
    \item GPU: Nvidia G-Force GTX 1080 via CUDA 11.7
\end{itemize}

\subsection{Classifier}
\label{exp/classifiers}

The classifier architecture in our analyses is an MLP consisting of two 50-node linear layers each followed by a $\relu$, then one last linear layer outputting $\outputdim$ logits.

In \autoref{fig:confusion_matrices} we show the test predictions of our classifier architecture, trained on each dataset with the batch size given in \autoref{tab:datasets}.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
\includegraphics[width=\textwidth]{./05_confusion_matrices/cake_on_sea/confusion_matrix_svg-tex.pdf}
        \caption{Accuracy on \CakeOnSea.}
\label{fig:cos_confusion_matrix}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
\includegraphics[width=\textwidth]{./05_confusion_matrices/forest_cover/confusion_matrix_svg-tex.pdf}
        \caption{Accuracy on \ForestCover.}
    \end{subfigure}

    \begin{subfigure}[b]{0.4\textwidth}
        \centering
\includegraphics[width=\textwidth]{./05_confusion_matrices/wine_quality/confusion_matrix_svg-tex.pdf}
        \caption{Accuracy on \WineQuality.}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
\includegraphics[width=\textwidth]{./05_confusion_matrices/online_news_popularity/confusion_matrix_svg-tex.pdf}
        \caption{Accuracy on \OnlineNewsPopularity.}
    \end{subfigure}

    \caption{Accuracy of our MLP classifier on each dataset.}
    \label{fig:confusion_matrices}
\end{figure}

It seems the classifier achieves low accuracy on \WineQuality{} and \OnlineNewsPopularity{} in particular.
These datasets are originally designed for regression, so they are perhaps not well suited for classification tasks; however, their size is also likely a factor: they have on the order of $10^2$ training examples per dimension where the others have on the order of $10^4$.

All datasets also have significant class imbalance: in \ForestCover{} the test set contains 56382 points with true class 1 while only 554 points in class 3.

\subsection{Autoencoder}

We build a normalizing flow model according to the NICE architecture described in \autoref{bg/nf} \cite{dinhNICE2015}.
The model is trained according to a Gaussian prior distribution, and its layers are 4 additive coupling layers where we divide the input into the odd columns and the even columns, and the nonlinearity is an MLP with 2 layers of 20 nodes each.

\section{Metrics}

In this section we describe the various metrics that will be used in some or all of our experiments.

\subsection{Validity}

Validity is the one requirement where the metric of interest is unambiguous: either a path is valid or it is not.
Hence, for testing a path generation method on some test set, we measure the validity rate, which is the proportion of valid paths.

\subsection{Computational performance}

Ensuring that CFs generation is fast is an important concern for stakeholders, especially if one needs to generate paths for many points of interest.
Hence, we measure the time taken \emph{for one step of one path}.
This is because depending on the number of steps chosen by the user, a path can take longer to generate, and methods such as \ls{} can take advantage of optimizations to compute many paths at once.

\subsection{Realism}

Following the reasoning presented in \cite{vanlooverenInterpretable2021}, we measure realism using a generative model.
However, while \citeauthor{vanlooverenInterpretable2021} use a VAE and thus estimate realism using the reconstruction error, we can compute the likelihood directly.
Indeed, normalizing flows are typically trained to minimize the $\nll = - \log P(x)$, so we can compute it easily.
$P(x)$ is in non-negative so the $\nll$ is in $\R_+$ and the closer it is to 0, the more realistic the point is deemed to be.

For some time we also used the local outlier factor \cite{breunigLOF2000}, for which an implementation is provided by the \sklearn{} Python package, but it did not scale to all of our datasets.

\subsection{Extending metrics to a path}

Many metrics take a point $x' \in \inputspace$ as input and return a score, but we wish to score a whole path $(x_t)_{t=1}^T$.
Hence, given a metric, we aggregate over the path (that is, from its starting point to the first step at which $\target$ is reached).

For this, we first produce paths in the form of a 3-dimensional tensor in $\R^{\nbsteps \times \batchsize \times \inputdim}$.
We compute the model predictions resulting in a 2-dimensional tensor in $\setclasses^{\nbsteps \times \batchsize}$.
Based on this and the target classes we generated in $\setclasses^\batchsize$, we construct a mask $\pathmask \in \{0, 1\}^{\nbsteps \times \batchsize}$ that highlights the valid paths:
if path $j$ is not valid, then
$\forall i \in \{1, \ldots, \nbsteps\}$ ${\pathmask_{ij} = 0}$, and if path $j$ is valid then $\pathmask_{ij} = 1$ up to and \emph{not including} the first index where $\target$ is reached, and 0 after that.

Then we can take the element-wise product of, \eg{} the $\nll$ computed for all points of all paths, to mask the unwanted points.

Note that if no paths are valid, the resulting product will be equal to zero, which can be misleading: $\nll = 0$ is a perfect score.
For this reason we take care to always report the validity rate alongside any metric computed this way.

Let us now describe our experiments in detail.

\section{Experiment 1: Validity losses}

In this experiment, we seek to determine the validity loss that maximizes validity rate among a number of choices.
For this we measure the validity rate over CFs generated from the test set across several CF methods and losses.

\paragraph{Description}
\label{validity_losses/description}

For a given dataset, we train one classifier.
Then for this dataset, we train several autoencoders all with the same architecture and with different seeds to average out random effects due to model initialization and dataset train-test split.
The seeds are generated uniformly based on one primary seed.

For a given dataset, we measure the validity rate across the whole test set.
That is, for every test point, we run the classifier to get the $\source$ class ($\source = \arg\max_{c \in \setclasses} f(x)$), and we sample the target class uniformly from $\setclasses \backslash \{\source\}$.
Then we generate a path using the given loss $\lossval$, and count it as valid or not.
It is important to sample the $\target$ uniformly to get a full understanding because, even when the label distribution is not uniform, in general there is no reason to prefer explanations for a given target rather than another.

We average the rates over the different seeds, and report the standard error.

\paragraph{Baselines}

The methods we use to produce the paths are variations on \ls{} and \revise{}.
Specifically, we run both methods twice: once with regularization on the $L_1$ distance, with $\lambda_\text{distance} = 0.3$ and one without regularization (with $\lambda_\text{distance}$ set to 0).
In the future using some automatic tuning procedure would be best: in our case $\lambda_\text{distance} = 1$ led to immobile paths whereas $\lambda_\text{distance} = 0.3$ could be too close to performing no regularization at all.

In fact, our implementation of both \ls{} and \revise{} is a joint abstraction that takes a parameter for the frequency of the gradient computation: for \revise{} this is set to 1 (\ie{} recompute the gradient after every step) while for \revise{} it is null (\ie{} never recompute it).
This way the number of steps and step size can be set to the same values with confidence.
However, this means that we do not run \revise{} using a more sophisticated optimization algorithm like \method{Adam} or \method{RMSprop}.

The losses compared in this experiment are as follows:
\begin{itemize}
    \item The probability-based losses: $\PrbTarget, \PrbSource{}$
    \item The log-probability-based losses: $\LogPrbTarget, \LogPrbSource{}, \LogPrbOthers{}$
    \item The logit-based losses: $\LogitTarget, \LogitSource{}, \LogitOthers{}$
\end{itemize}
where we test all losses that can be parametrized with the default parameter of $\lambda = 1$ and also with $\lambda = 0.1$, for a total of 13 losses ($\PrbTarget$ is not parametrized, and recall that $\PrbOthers{}$ is equivalent to $\PrbTarget$.).
Here again, a larger scale parameter search should have been done, but instead we first select the best-performing loss and then show the progression for $\lambda \in [0, 1]$.

\paragraph{Datasets}

We run our experiment on the datasets described in \autoref{sec:datasets}, namely \CakeOnSea, \ForestCover, \WineQuality{} and \OnlineNewsPopularity.

The batch sizes are chosen so that one epoch takes less than 10 steps, except for \ForestCover{} where the size of the train set would have resulted in very large batches.
The final numbers are:
\begin{itemize}
    \item \CakeOnSea: 12000
    \item \ForestCover: 25000
    \item \WineQuality: 500
    \item \OnlineNewsPopularity: 3000
\end{itemize}

\paragraph{Metrics}

The one metric in this experiment is the validity rate, \ie{} the proportion of paths that reach their target class across the whole test set.

\paragraph{Results}

The results for each dataset and loss are given in \autoref{tbl:validity_losses}.
We highlight in gray the five losses that yield the best average validity rate across all path methods; the results are not uniform across datasets so we resort to this rough estimate to decide between the losses.

\begin{table}[h!]
\centering
\subfile{../Figures/05_validity_losses.tex}
\caption{Validity rate means with their standard error. Highlighted are the five losses with the best average results.}
\label{tbl:validity_losses}
\end{table}

As expected, the validity rate achieved by \revise{} is generally much more acceptable than that of \ls{}, across all losses.
$\LogitSource{}$ is always among the best validity losses.
However, it is not always clear which value of $\lambda$ should be used.
Hence, we show the validity rate for $\lambda \in \{ 0.01, 0.05, 0.1, 0.2, 0.5, 1\}$ in \autoref{fig:lambda_progression}.

\begin{figure}[htbp]
    \centering
\includegraphics[width=0.7\textwidth]{./05_logit_source_lambda_progression}

    \caption{Validity rate for $\LogitSource{\lambda}$ for different values of $\lambda$.}
    \label{fig:lambda_progression}
\end{figure}

From this plot it is clear that in general letting $\lambda = 1$ leads to higher validity rate;
however the validity is unusually high for $\lambda = 0.1$ compared to the others.

To make better sense of our findings, we also plot our various losses and their gradients on \CakeOnSea.
Here, we select plots for some losses and where the target class is always class 2.
For reference, the full plots are shown in \autoref{ch:plots}.


\begin{figure}[htbp]
    \centering
\includegraphics[width=\textwidth]{./05_cf_losses/cf_losses_detail}
    \caption{Loss profile for various losses on \CakeOnSea, with ${\target = 2}$.}
    \label{fig:cf_losses_detail}

    \vspace*{\floatsep}% https://tex.stackexchange.com/q/26521/5764

\includegraphics[width=\textwidth]{./05_cf_losses/cf_gradients_detail}
    \caption{Opposite of the loss gradient for various losses on \CakeOnSea, with $\target = 2$.}
    \label{fig:cf_gradients_detail}
\end{figure}

\paragraph{Interpretation}

From the test accuracies shown in \autoref{fig:cos_confusion_matrix}, we know that for \CakeOnSea{} the prediction probabilities are all close to 1, except perhaps near the decision boundaries.
Hence, we can verify our intuitive thinking from \autoref{sec:validity_losses}: for the probability-based losses, the loss is very flat, \ie{} the gradient is close to zero.
For $\LogPrbOthers{}$ it seems the logits from the other classes counteract the target logit, so that paradoxically the paths get pushed away from class 2 in places.
For the logit-based losses however, the behavior is fairly uniform across the whole data distribution, and so the gradients clearly point the path towards class 2.

This leads us to choose $\LogitSource{\lambda=1}$ as validity loss in the rest of our analyses.

\section{Experiment 2: Path regularized training}
\label{exp/path_reg}

Having determined an appropriate validity loss to use for generating valid paths, we turn now to our objective of interpretability.
In this experiment we seek to determine if \ls{} equipped with a path regularized autoencoder can perform on the same level as \revise{}, increasing interpretability without compromising validity.

Our proxy for complexity of a path is \emph{the number of class boundary crossings}:
the path should ideally transition from $\source$ to $\target$ without going through other classes.
We thus add our $\BoundaryCrossLoss$ to the $\nll$ when computing the loss of the autoencoder.
Our final loss for training the autoencoder is
\begin{align*}
    \loss(\phi) = \nll(\phi) + \BoundaryCrossLoss(\apath)
\end{align*}
where $\apath$ is a path generated with some CF method.


\paragraph{Description}

First, we generate some seeds.
For a given dataset, we train one classifier, as in \autoref{validity_losses/description}.
For each seed, we train several autoencoders: one without path regularization, and then one per CF generation method using the path regularized training procedure.

Then, each autoencoder is used to produce explanation paths on the whole test set, like in \autoref{validity_losses/description}, and metrics are computed with the paths as input.
Finally, for each explanation method, we aggregate the metrics obtained for each seed and report the standard error.

The paths are 100 points long (including the starting point, so the number of steps is 99) and the step size $\lambda_\text{LS}$ is 0.1. We found this to be satisfactory so that the path is long enough to reach the target class, yet with step size short enough that the path does not go too far too quickly.

\paragraph{Baselines}
\label{exp/path_reg/baselines}

Since the distance parameter seems inconsequential from our previous results,
in this experiment we use \ls{} and \revise{} in their basic form, \ie{} with no distance regularization for \ls{} and with $\lambda_\text{distance} = 0.3$ for \revise.

\paragraph{Datasets}

For memory performance reasons we adapt the batch size for each dataset.
We compute the batch size so that one training epoch (one pass through $\trainset$) takes 20 steps, except for \ForestCover{} where the batches are still too big.
The final numbers are:
\begin{itemize}
    \item \CakeOnSea: 2881
    \item \ForestCover: 5000
    \item \WineQuality: 195
    \item \OnlineNewsPopularity: 1190
\end{itemize}

\paragraph{Metrics}

We measure the validity rate, as before, as well the number of boundary crossings \emph{before reaching $\target$} (so if the path goes directly from $\source$ to $\target$ the value is 0) which is the proxy for interpretability we chose.

For measuring realism, we use a specially-trained autoencoder with the same architecture as the others, trained using only the $\nll$ as a loss.
The realism score for $\CF{x}$ is given by its $\nll$ as computed by our special autoencoder.

We also measure the computational complexity (the duration for one computing one step of one path).

\paragraph{Results}

The results for each dataset and method are given in \autoref{tbl:path_reg}.

\begin{table}[h!]
    \centering
    \subfile{../Figures/05_path_reg.tex}
    \caption{Path metrics with their standard error. ($\uparrow$) indicates higher is better, ($\downarrow$) indicates lower is better.}
    \label{tbl:path_reg}
\end{table}

It seems the regularization did not achieve its objective, as the mean number of boundary crossings stays approximately the same regardless of whether or not regularization was performed.
The only case where an effect is observed is for \revise{} on \CakeOnSea{}.
In no instance do we see a decrease in validity rate when applying path regularization.
Similarly, path regularization by and large has no effect on realism, except for \ls{} on \CakeOnSea{} and \ForestCover{} where the $\nll$ is decreased when applying regularization.
The time per step does not depend on the autoencoder training, only on the architecture.
Yet, we can observe the difference in time performance between \ls{} and \revise{}: \ls{} is up to 70 times faster than \revise{} on \OnlineNewsPopularity{} (recall our paths are 100 steps long).

\paragraph{Interpretation}

The regularization was not strong enough to alter the training of the autoencoder.
Recall our loss function is given by
\begin{align*}
    \BoundaryCrossLoss(\apath) =
    - \frac{1}{\nbsteps} \sum_{t=1}^{\nbsteps}
    \max \{ p^{(t)}_\source, p^{(t)}_\target \}
\end{align*}
which is in $[-1, 0]$.
By contrast, the $\nll$ is computed according to a Gaussian prior distribution which can typically be as large as $\inputdim$ (see \autoref{methods:0_sols}), which is larger than 10 for all datasets except for \CakeOnSea{} where $\inputdim=2$.
Therefore, one easy improvement is to multiply the $\BoundaryCrossLoss$ by $\inputdim$.

\section{Experiment 3: Robustness of explanations with path regularization}

Given that we were able to achieve more interpretable paths using path regularization, we ask whether it is possible to meet any interpretability criterion, even a nonsensical one.
For example, can we use path regularization so that all paths pass through one designated point, regardless of $x$ and $\target$?
To do so, we select a point $\xgoal \in \inputspace$ that we can reasonably expect to be unrealistic, but is not completely out of reach compared to points in the train set.
Then, we devise a path loss that encodes the distance from a path to $\xgoal$, and let our path regularization training enforce this constraint.

\paragraph{Description}

We perform path regularized training using the adversarial path loss described in \autoref{methods:robustness}.
The loss encodes how close a path passes by a given point $\xgoal$, which is potentially unrealistic.
In our experiment, we pick a $\xgoal$ that is easy to compute, likely unrealistic yet not completely out of bounds for the dataset:
\begin{equation}
    \xgoal = (\max\{ x_i : x \in \trainset \})_{i=1}^D
\end{equation}
\ie{} $\xgoal_i$ is the largest value encountered in the dataset for feature $i$.

Apart from this the setup is the same as that described in \autoref{exp/path_reg}, where we compare paths with the distance path regularization and without.

\paragraph{Baselines}

We use the same baselines as in the previous experiment (\autoref{exp/path_reg/baselines}).

\paragraph{Datasets}

We use the same datasets as in the previous experiments.

\paragraph{Metrics}

As before, we analyze the trade-off that is incurred by applying path regularization.
We measure the validity rate, as well as the realism score and the path loss.

Since we seek to minimize the distance from a path to $\xgoal$, we  include our path distance loss as a measure.
Yet, it is important that manipulated paths retain the validity quality for the attack to be successful, hence we measure validity as well.

\paragraph{Results}

The results for each dataset and method are given in \autoref{tbl:robustness}.

\begin{table}[h!]
    \centering
    \subfile{../Figures/05_robustness.tex}
    \caption{Path metrics with their standard error. ($\uparrow$) indicates higher is better, ($\downarrow$) indicates lower is better.}
    \label{tbl:robustness}
\end{table}

\paragraph{Interpretation}



\end{document}