\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}

\begin{document}

\chapter{Previous work}
\label{ch:previous_work}

In this chapter we describe some methods for counterfactual explanation (CFX) generation and related topics that were of interest during this project.
First we present some areas of research that are closely linked to CFX generation, then we provide an overview on methods that rely on generative models to promote realistic CFX.
A comprehensive review of many other CFX generation methods can be found in \cite{vermaCounterfactual2020}.

\section{Related topics}

\subsection{Contrastive explanations}

Contrastive explanations are close enough to CFX that they are often dealt with together: \citeauthor{stepinSurvey2021} even bring them together under the name ``contfactual'' \cite{stepinSurvey2021}.

Where counterfactual explanations are virtual points close to the explained input, potentially highlighting underlying concepts which are deemed important by the classifier, contrastive explanations are ratings of the importance given by the classifier of a given \emph{user-defined} concept to a given class.
Hence, contrastive explanations do not suffer from the problem of semanticity of explanations that was raised in \autoref{intro/semanticity_problem}, but in return it is up to the user to propose meaningful concepts to evaluate.

Accordingly, \citeauthor{jacoviContrastive2021} develop a method for text data that evaluates how much a given concept contributes to a certain class compared to another \cite{jacoviContrastive2021}.
For instance, they train a model for the ``natural language inference'' task: given two sequential sentences as input, the model should output whether the first sentence entails, contradicts or is neutral with respect to the second sentence.
Then, given a concept of their choosing, \eg{} the amount of overlapping words between the two sentences, they can compute a metric of contrastive importance of this concept between two classes.
In the case of word overlap, they validate that in general this concept allows the model to differentiate between entailment and neutrality.

\citeauthor{baiConcept2022} propose ``concept gradients'' (CGs) \cite{baiConcept2022}, a generalization of a pre-existing technique called ``concept activation vectors'' (CAVs) \cite{kimInterpretability2018}, to evaluate how much a concept influences the model behavior, which gives an indication of whether the model makes use of this concept in its prediction.
More precisely, concepts are evaluated using the correlation of gradients between a corresponding concept vector and the model outputs; however, CAVs assume that the concept can be expressed as a linear function, whereas CGs have no such requirement and are therefore capable of evaluating more flexible concepts.

\subsection{Causal learning and counterfactuals}

Counterfactual statements are closely related to causal inference \cite{morganCounterfactuals2015}.
This link is formally made in Pearl's theory of \emph{structural causal models} (or SCMs for short): given a complete formal description of the causal relationships between variables, the theory describes a natural procedure to compute how a change in one variable would influence the others \cite{pearlCausal2016}.

\citeauthor{karimiAlgorithmic2020} develop this relationship in the context of CFX in the form of a lower bound statement: given some cost function, if the true SCM is known fully, then the Pearl procedure yields a CF that has minimum cost \cite{karimiAlgorithmic2020}.
In other words, any other proposed CF will in some way neglect some variable relationships, and hence include redundant perturbations which increase the cost.

This brings us to the topic of causal discovery: the problem of extracting a causal graph from (usually observational) data.
An introduction to classical methods can be found in \cite{cunninghamCausal2021}, and a review of the literature on ML-powered techniques up to 2022 is presented in \cite{vowelsYa2023}.
As mentioned, the full causal graph for a given task is a powerful tool which enables all kinds of inference to be carried out.
However, the data used for training machine learning models seldom includes causal information about how the data were obtained, because real-world data collection processes often do not allow for causal experiments to take place, be it for physical (\eg{} in economy) or ethical reasons (\eg{} in medicine).
Hence, neural network models (and statistical models more generally) can only uncover \emph{correlations} between variables: the true form of the causal graph can only be known up to undirected edges between variables and possibly some unobserved latent variables.
% not to mention that the selection process can include bias that is not explicitly declared

\subsection{Semantic latent features}

When building a latent variable generative model, the idea of being able to tune human-understandable parameters is appealing: for example, when generating pictures of human faces, one might want to generate a smiling face in particular, or a face of a person wearing glasses.
One way to achieve this is to encode into the latent space the concepts of ``smiling'' and ``glasses'', so that a particular direction in latent space represents a given concept.
Then, given an image latent $z$, we can increase its ``smiling factor'' by perturbing $z$ in the direction representing the concept of smiling.

Thus, some research has been done to promote this encoding of semantic information, and also to analyze whether some popular generative models have this semanticity property even when not explicitly trained for it.

For example, recent work demonstrates this exact kind of manipulation on a diffusion model \cite{kwonDiffusion2023}.
In this work, the authors detail what they mean by semanticity:
\begin{description}
	\item[Concepts are universal] If you perturb two inputs in the direction of the same concept $c$, then in both cases the result should exhibit $c$.
\item[Concepts scale] If you perturb in the direction of $c$ by 2 units, the result will exhibit the concept more than if you perturb by 1 unit.
\item[Concepts add] If you perturb in a direction that is a combination of those representing concepts $c_1$ and $c_2$, then the result will be modified in these two ways at the same time.
	\item[Non-destructiveness] Perturbing in a semantic direction does not result in an unrealistic (``broken'') point.
\end{description}

This description is reminiscent of adversarial perturbations, and in particular the universality of semanticity is challenged in \cite{moosavi-dezfooliUniversal2017}.
In this work, the authors demonstrate that, given a conventional deep learning model, it is possible to find \emph{one} small constant perturbation which, when applied to \emph{any} input in the dataset, leads to a change in model prediction.

Yet, for many applications it is often sufficient to require weaker properties of latent features, such as disentanglement, efficiency or non-spuriousness.
\citeauthor{wangRepresentation2022} bridge the gap between latent representations and causal learning by formulating these properties in terms of causal quantities; because these quantities are difficult to compute in general, they derive a metric approximating the degree of efficiency and non-spuriousness, which can be used to regularize representation learning models \cite{wangRepresentation2022}.
This can only be used in the supervised learning setting, where a label is known and can be used for training.
In fact, \citeauthor{locatelloChallenging2019} demonstrate that achieving a disentangled representation is not possible without labels \cite{locatelloChallenging2019}.

\section{CFX generation methods}

We now present some previous works on CF generation methods, covering in particular those that rely on generative models (all except $\method{DiCE}$).

% - [ ] wachterCounterfactual
% - [x] REVISE (@joshiRealistic2019, gradient in latent space)
%   - gifsplanation, more basic and applied to images (@cohenGifsplanation2022)
% - [x] Prototypes (CFPrototype, @vanlooverenInterpretable2021)
% - There are repositories containing implementations
% and benchmarks
%   - [ ] @pawelczykCARLA2021
%   - @deoliveiraFramework2021
% - [X] CLUE (to reinforce model certainty)
% - [x] CRUDS
% - Causality-related
%   - More sensible if recourse is a desired goal.
%   - [ ] @mahajanPreserving2020
%   - [x] @karimiAlgorithmic2020
% - [ ] CARE
%   - uses genetic algo
% - [x] DiCE
%   - Determinantal point processes
% -generate a set number of CFs, $k$.
% - gradient descent over all of them at the same time, using DPP diversity regularization (determinant of kernel matrix given some distance)
% - [ ] CERTIFAI
% - [ ] GRACE

% Using VAE: \cite{nguyenLearning2020}


\subsection{Methods without generative models}

\paragraph{CF-Rule duality}

\citeauthor{gengComputing2022} prove a duality theorem linking rule-based and CF explanations, assuming that certain properties hold on the CFX \cite{gengComputing2022}.
They exploit this duality in one direction by devising two methods for extracting rule-based explanations from CFX.
In doing so, they demonstrate that a great many methods in the literature do not fit the requirements for producing proper rules; namely, most of them do not have the ability to specify logical feasibility constraints on the CFX.

\paragraph{\method{DiCE}}

\citeauthor{mothilalExplaining2020} generate a diverse set of CFX by gradient descent, solving for $k$ candidate CFX at the same time \cite{mothilalExplaining2020}.
Diversity is modeled by a metric taken from the study of determinantal point processes, which is based on the pairwise distances between the $k$ CFX.
Using this constraint, they succeed at keeping a high rate of valid CFX while increasing diversity.

\paragraph{Wachter \textsl{et al.}}

We described the gradient-descent procedure presented by \citeauthor{wachterCounterfactual2017} \cite{wachterCounterfactual2017} in \autoref{intro/wachter};
the other gradient-descent-based methods we will now describe are inspired by this work.

\subsection{Methods with generative models}

\paragraph{\revise}
\label{par:revise}

\citeauthor{joshiRealistic2019} follow the strategy outlined by \citeauthor{wachterCounterfactual2017} in that they frame the CFX search as a constrained optimization problem and approach it with gradient descent:
If $x^{*}$ is an example with true label $y^{*} = 0$ then they find a CFX $x'$ by solving:
\begin{align*}
	x' = & \arg\min_{x} c(x, x^{*})              \\
	     & \text{such that } f(x') = y' = 1
\end{align*}
where $c$ is some cost function that is situation-dependent.
However, in order to ensure the realism of the CFX, they perform the optimization in a latent space learned through a VAE.

For example, they use as a cost function the $L_1$ distance between the original input $x^{*}$ and the candidate counterfactual $x'$.
Their VAE is denoted as $(\mathcal{F}, \mathcal{G})$.
Thus, letting $z' = \mathcal{F}(x')$, and $\ell$ a loss function like the one used for training $f$ (such as the cross-entropy), the loss that is minimized is the following:
\begin{equation}
	\ell(f(\mathcal{G}(z')), 1)
	+ \lambda_\text{distance} \norm{\mathcal{G}(z') - x}_1
\end{equation}
The resulting CFX generation algorithm is called \revise{} \cite{joshiRealistic2019}.

\paragraph{\method{CFProto}}

\citeauthor{vanlooverenInterpretable2021} also use a VAE to ensure realism, but they follow a different strategy \cite{vanlooverenInterpretable2021}.
For a given $x$, they build a list of ``prototypes'', one per class, which are interpreted as typical examples of those classes.
The prototype of class $c$ for $x$, $\text{proto}_c$, is the centroid of the $k$-neighborhood of $\enc(x)$: the average latent over the $k$ points predicted as $c$ closest to $\enc(x)$.
Then, the optimization algorithm for finding $\CF{x}$ pushes the path towards the closest prototype.
The reasoning is that $\text{proto}_c$ is likely realistic, because it is an average of real points, and it is likely of class $c$, for the same reason; therefore, the closer $\CF{x}$ is to a prototype, the more realistic and valid it is likely to be.

\paragraph{Diffeomorphic Counterfactuals}
\label{previous_work:diffeo}

\revise{}, which we discussed in \autoref{par:revise}, relies on a VAE to promote realistic CFX.
However, this model is in general not invertible, so that there is a loss of information when sending inputs through it.
This is problematic when it comes to trustworthiness of the explanations: if the autoencoder is not trained well enough, the latent space will not accurately
model the data distribution and the resulting CFX will be of little use.

For this reason, in \cite{dombrowskiDiffeomorphic2021} \citeauthor{dombrowskiDiffeomorphic2021} use normalizing flows instead, which are invertible by design.
Furthermore, they draw on differential geometry to prove that if the NF is well-trained, then the CFX will stay within the data manifold.

They show that for multi-class problems on images, their method achieves sensible counterfactuals compared to simply perturbing the inputs in input space.

\paragraph{\method{CLUE}}

\citeauthor{antoranGetting2021} introduce \method{CLUE} \cite{antoranGetting2021}, a method for producing counterfactuals that are as realistic as possible by way of a conditional VAE. The method is very similar to \revise: the loss function that is minimized to perturb $x$ via gradient descent is
\begin{equation}
    \loss(z') = h(y | \dec(z')) + \norm{\dec(z') - x}_1
\end{equation}
where $h$ is some measure of ``uncertainty'' (what we call realism) of $\dec(z')$ with respect to a target label $y$.
Their approach to evaluation of CFX is intriguing:
to get a known ground-truth data distribution without resorting to purely
fabricated datasets, they train a ``ground-truth'' VAE on a real dataset (the MNIST hand-written digits dataset);
the VAE is then used to generate a new artificial dataset, and the method is tested on these \emph{artificial} data where
metrics such as the ``ground-truth'' log-likelihood can be measured.

Importantly, the authors validate their findings with a user study, which is not frequent in works on CFX.

\subsection{Methods with class-aware generative models}

\paragraph{\method{CRUDS}}

\citeauthor{downsCRUDS2020} make use of a conditional subspace VAE (or CSVAE for short) to adapt the latent representation to the classification task:
the latent space is decomposed (or ``disentangled'') into a product of small spaces: $\latentspace = \mathcal{Z} \times \prod_{i} \mathcal{W}_i$, where each space $\mathcal{W}_i$ contains information on the features most correlated with label $i$ \cite{klysLearning2018}.
For an input $x$, they let $\enc(x) = (z, w)$ where $z$ is not sensitive to the label and $w$ is sensitive to the label; thus, by altering only $w$ based on the target class, they can then form a counterfactual latent as $\CF{z} = (z, \CF{w})$ and let $\CF{x} = \dec(\CF{z})$.
In fact, by sampling several $\CF{w}$ they can obtain a whole set of CFX that are uniquely different from $x$.

As a side note, recall the recent proposal for the Union of manifolds hypothesis \cite{brownUnion2022} we mentioned in
\autoref{intro/image_data}: this method fully takes advantage of that fact.

\paragraph{VAE-NF blend}

\citeauthor{zhangInterpretable2022} train the autoencoder conjointly with the classifier, so that the latent representation used for CFX generation is also used for classification \cite{zhangInterpretable2022}.
In particular, the latent representation architecture consists of normalizing flows within a VAE, an idea first developed in \cite{rezendeVariational2015}.

\subsection{Summary of methods}

We summarize the presented works in terms of which generative model is used (if any) and whether or not the generative model
is trained using the class labels in some way.
We also mention when a work does not specifically target or test their method on tabular data problems.

\begin{table}[htbp]
	\centering
	\begin{tabular}{lccc}
		\toprule
		                                                     & Tabular data & Generative model                       & Class-aware \\
		\midrule
		\method{CFProto} \cite{vanlooverenInterpretable2021} & Yes          & VAE                                    & No          \\
		\method{CLUE} \cite{antoranGetting2021}              & Yes          & VAE                                    & No          \\
		\method{CRUDS} \cite{downsCRUDS2020}                 & Yes          & CSVAE                                  & Yes         \\
		Diffeo.~CF \cite{dombrowskiDiffeomorphic2021}        & No           & RealNVP                                & No          \\
		\revise{} \cite{joshiRealistic2019}                  & Yes          & VAE                                    & No          \\
		VAE-NF \cite{zhangInterpretable2022}                 & Yes          & VAE$+$NF \cite{rezendeVariational2015} & Yes         \\
		\midrule
		\method{DiCE} \cite{mothilalExplaining2020}          & Yes          & None                                   & NA          \\
		Rule-CF duality \cite{gengComputing2022}             & Yes          & None                                   & NA          \\
		Wachter \textsl{et al.} \cite{wachterCounterfactual2017}             & Yes          & None                                   & NA          \\
		\bottomrule
	\end{tabular}
	\caption{Summary of CF generation methods we presented, in alphabetical order.
	``Tabular data'' refers to whether or not the work specifically addresses tabular data; ``Class-aware'' refers to whether or not the generative model is trained with access to class labels or similar information.}
\end{table}

\section{Metrics for CFX}

In general, measuring the quality of an explanation is not a straightforward endeavor.
However, in our case we have at least one precise goal: to change the class to a target class.
When this is achieved, we say that the explanation is \emph{valid}.
Hence, our first and foremost metric is the \emph{validity rate}: the fraction of CFX that reached the target.

When a method produces several CFX, it is often desirable to guarantee \emph{diversity}, so that the proposed CFX are not all the same.

\subsection{Distance}

For a CFX to be relevant to the explained input, it should be close enough to it; thus some notion of \emph{distance} is often used, \eg{} the Euclidean ($L_2$) norm.
Furthermore, it is often preferred to have \emph{sparse} perturbations, in the interest of simplicity; for this the $L_1$ loss is a common choice \cite{joshiRealistic2019,antoranGetting2021}.
\citeauthor{vanlooverenInterpretable2021} even propose a linear combination of the two \cite{vanlooverenInterpretable2021}.

\subsection{Feasibility}

When a CFX is intended to provide recourse, authors tend to mention \emph{feasibility} as a requirement.
This refers to the possibility for, \eg{} a user, to actually follow the recourse path proposed in the form of a CFX.
% Several metrics aim to capture feasibility, including:

For example, \citeauthor{pawelczykLearning2020} introduce metrics based on \emph{percentile shift}, that is, for feature $i$, how far $\CF{x}_i$ has shifted in terms of percentile compared to $x_i$ \cite{pawelczykLearning2020}.
For example, letting $\text{percentile}_i^\data(x) \in [0, 1]$ denote the percentile of $x$ for feature $i \in \{1, \ldots, \inputdim\}$, they define
\begin{align*}
    \text{\emph{total percentile shift} as }   & \sum_{i=1}^\inputdim \abs{ \text{percentile}_i^\data(\CF{x}) - \text{percentile}_i^\data(x) }                  \\
    \text{\emph{maximum percentile shift} as } & \max_{i \in \{1, \ldots, \inputdim\}} \abs{ \text{percentile}_i^\data(\CF{x}) - \text{percentile}_i^\data(x) }
\end{align*}

Before them, \citeauthor{wachterCounterfactual2017} already considered a metric of distributional shift, which is based on the median absolute deviation for feature $k$ \cite{wachterCounterfactual2017}:
\begin{align*}
	d(x, x') = \sum_{k \in F} \frac{\abs{x_k - x'_k}}{\text{MAD}_k}
	 \quad \text{where} \quad
	\text{MAD}_k
	= \text{median}_{j \in P} \left\{
	\abs{
		x_{j,k} - \text{median}_{l \in P} \{ x_{l, k} \}
	}
	\right\}
\end{align*}
where $P$ is the set of data points, $F$ the set of features and $x_{j, k}$ is feature $k$ of point $x_j$.

\subsection{Realism}

Measuring realism of a data point is a topic of research in itself and a number of metrics have been used in the CFX literature.
Some make use of mathematically justified estimators, such as the \emph{maximum mean discrepancy} (or MMD for short) \cite{zhangInterpretable2022}, which can be cumbersome to compute, while others take advantage of generative models, \eg{} using the reconstruction error of an autoencoder \cite{vanlooverenInterpretable2021}.
Other metrics such as \emph{proximity} or \emph{connectedness} \cite{laugelIssues2019} are derived from the distance between points and draw from older metrics such as the \emph{local outlier factor} \cite{breunigLOF2000}.

Now that we have covered some of the recent advances on CFX methods powered by generative models, let us present our original work.

\end{document}