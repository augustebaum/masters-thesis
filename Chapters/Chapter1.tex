\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}

\begin{document}


\chapter{Background} % Main chapter title
\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

\newcommand{\method}[1]{\texttt{#1}}
\newcommand{\citenote}{\textbf{CITE}}

%----------------------------------------------------------------------------------------
% \begin{enumerate}
%     \item Background (Context, Definitions)
%     \item Previous work (Related works, Particularly timely or relevant work)
% \end{enumerate}

\section{Context}

\subsection{Explainable AI}
% \newacronym{xai}{XAI}{Explainable AI}

The term "Explainable AI" refers to the endeavour of extracting human-friendly \emph{reasoning} from an AI system, either by a post hoc procedure or by guiding its learning process to give it the ability to produce said reasoning \citenote{}.

Some of the reasons driving research in explainable AI (referred to as XAI hereafter) are summarized in \cite{zhangSurvey2021}:
\begin{itemize}
    \item Scientists that use AI hope to recover human-understandable information pertaining to their problem of interest (e.g. in drug design or physics);
    \item Regulators hope to make AI-based decisions accountable, especially those that affect data subjects directly (e.g. in insurance, banking, or in criminal justice);
    \item AI practitioners hope to limit unexpected failure scenarios in their work, especially in cases where failure can be costly (e.g. in medicine or finance).
\end{itemize}

It has been argued \cite{rudinWhy2019} that in many cases (including in the tabular setting) trying to "explain" black-box models is less productive than using more interpretable-by-design techniques.

inherently interpretable models should be encouraged rather than relying on new techniques to "explain" black-box methods.

Because of how ubiquitous AI is becoming, XAI research is also blossoming and the research field is vast.
Accordingly, some taxonomies have emerged recently that help differentiate methods \cite{zhangSurvey2021, bellePrinciples2021}.

One key characteristic of an XAI method is its \emph{scope}: a \emph{global} XAI method applies to a \emph{whole} fully trained ML model, while a \emph{local} method gives an explanation of \emph{one prediction} by the model.
Global methods report general information that does not necessarily reflect
how the model acts in all situations, while local methods produce results with
a more limited range, which can therefore be misleading.

% It's a trade-off: think of it as looking at just the mean vs. looking at just one point

In the following we focus on local explainability methods, relying on the assumption that aggregating many local explanations can help us grasp the model's general behaviour.

\subsection{Local explainability}
Having discarded all global methods, we can still subdivide local methods into several types of explanation. Some of them are outlined in \cite{bellePrinciples2021}:

\paragraph{Rule-based approaches}
One rule-based approach, called \method{Anchors}, consists in extracting simple rules that hold locally around a given explained input \cite{ribeiroAnchors2018}.
The advantage of rules as explanations is their intelligibility; it has been shown that rules depicting why a model behaved a certain way tend to be well understood compared to other types of explanation \cite{limWhy2009}.

\paragraph{Local approximation approaches}
One strategy is to transform the model into a simpler, more interepretable one such that the proxy model has the same local behaviour. In LIME \cite{ribeiroWhy2016}, a commonly used method, the proxy is a linear model.
More recently, it was shown that many pre-existing explanation methods, including LIME, can in fact be formulated as instances of the same framework called Local Function Approximation \cite{hanWhich2022}.

% CF is dual to rule-based \cite{gengComputing2022}.

\paragraph{Counterfactual approaches}
A counterfactual explanation (CFX hereafter) is one or more \emph{virtual points} that are close to the explained input, but for which \emph{the model prediction is different}.
They aim to answer the question: ``What would have to change in the input for the model to change its prediction?''
This is reminiscent of adversarial perturbations, in which a small perturbation can suffice to ``trick'' the model into making a nonsensical prediction.
However, in CFX one tries to keep the virtual points as realistic as possible.

CFX provide value in several ways:
\begin{itemize}
    \item They indirectly show which features are being used by the model in its decisions;
    \item They provide insight on the model's understanding of what makes an input point realistic;
    \item They constitute a way to provide \emph{recourse} when the model is used in a setting where recourse is required or desirable (for example in the context of a loan application).
\end{itemize}

\cite{wachterCounterfactual2017}

\end{document}