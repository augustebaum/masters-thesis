\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}

\begin{document}


\chapter{Background} % Main chapter title
\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{Plan for this section}
\begin{enumerate}
    \item Background (Context, Definitions)
    \item Previous work (Related works, Particularly timely or relevant work)
\end{enumerate}

\subsection{Explainable AI}
% \newacronym{xai}{XAI}{Explainable AI}
Some of the reasons driving research in explainable AI (XAI hereafter) are summarized in \cite{zhangSurvey2021}:
\begin{itemize}
    \item Scientists that use AI to achieve their performance goals hope to recover human-understandable information pertaining to their problem of interest;
    \item Regulators hope to make AI-based decisions accountable, especially those that affect data subjects directly (e.g. in insurance, banking, or even in criminal justice);
    \item AI users hope to limit unexpected failure scenarios in their work, especially in cases where failure is unacceptable.
\end{itemize}

The point has been made \cite{rudinWhy2019} that using inherently interpretable models should be encouraged, rather than relying on new techniques to "explain" black-box methods.

Because of how ubiquitous AI is becoming, XAI research is also blossoming and the research field is vast.
Some taxonomies have emerged recently that help differentiate methods \cite{zhangSurvey2021, bellePrinciples2021}.
% It's a trade-off: think of it as looking at just the mean vs. looking at just one point
One key characteristic of an XAI method is its \emph{scope}: a \emph{global} XAI method applies to a \emph{whole} fully trained ML model, while a \emph{local} method gives an explanation of \emph{one prediction} by the model.
Global methods report general information that does not necessarily reflect
how the model acts in all situations, while local methods produce results with
a more limited range, which can therefore be misleading.
Yet, assuming that aggregating many local explanations can help us grasp
the model's general behaviour, it can be tempting to only focus on those.

\subsection{Local explainability}
Having discarded all global methods, we can still subdivide local methods into several strategies. Some of them are outlined in \cite{bellePrinciples2021}:

\begin{itemize}
    \item Rule-based approaches (Anchors)
    \item Local approximation approaches (LIME, LFA)
    \item Counterfactual approaches (Wachter)
\end{itemize}

Rule-based 

CF is dual to rule-based

Counterfactual explanations (CFX hereafter) consist in producing \emph{virtual points} that are close to the explained input, but for which the model prediction is different.
They aim to answer the question: ``What would have to change in the input for the model to change its prediction?''
This is reminiscent of adversarial perturbations, in which a small perturbation can suffice to "trick" the model into making a prediction nonsensical.
In CFX the goal is to keep the virtual points as realistic as possible.

CFX provide value in several ways:
\begin{itemize}
    \item They indirectly show which features are being used by the model in its decisions;
    \item They provide insight on the model's understanding of what makes an input point realistic;
    \item They constitute a way to provide \emph{recourse} when the model is used in a setting where recourse is required or desirable.
\end{itemize}


\end{document}