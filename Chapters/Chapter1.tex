\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}

\begin{document}


\chapter{Background} % Main chapter title
\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

\newcommand{\method}[1]{\texttt{#1}}
\newcommand{\note}[1]{\textbf{#1}}
\newcommand{\citenote}{\note{CITE}}

\newcommand{\CF}[1]{#1^\text{CF}}
\newcommand{\target}{\text{target}}

%----------------------------------------------------------------------------------------
% \begin{enumerate}
%     \item Background (Context, Definitions)
%     \item Previous work (Related works, Particularly timely or relevant work)
% \end{enumerate}

\section{Context}

\subsection{Explainable AI}
% \newacronym{xai}{XAI}{Explainable AI}

The term "Explainable AI" refers to the endeavour of extracting human-friendly \emph{reasoning} from an AI system, either by a post hoc procedure or by guiding its learning process to give it the ability to produce said reasoning \citenote{}.

Some of the reasons driving research in explainable AI (referred to as XAI hereafter) are summarized in \cite{zhangSurvey2021}:
\begin{itemize}
    \item Scientists that use AI hope to recover human-understandable information pertaining to their problem of interest (e.g. in drug design or physics);
    \item Regulators hope to make AI-based decisions accountable, especially those that affect data subjects directly (e.g. in insurance, banking, or in criminal justice);
    \item AI practitioners hope to limit unexpected failure scenarios in their work, especially in cases where failure can be costly (e.g. in medicine or finance).
\end{itemize}

It has been argued \cite{rudinWhy2019} that in many cases (including in the tabular setting) trying to "explain" black-box models is less productive than using more interpretable-by-design techniques.

inherently interpretable models should be encouraged rather than relying on new techniques to "explain" black-box methods.

Because of how ubiquitous AI is becoming, XAI research is also blossoming and the research field is vast.
Accordingly, some taxonomies have emerged recently that help differentiate methods \cite{zhangSurvey2021, bellePrinciples2021}.

One key characteristic of an XAI method is its \emph{scope}: a \emph{global} XAI method applies to a \emph{whole} fully trained ML model, while a \emph{local} method gives an explanation of \emph{one prediction} by the model.
Global methods report general information that does not necessarily reflect
how the model acts in all situations, while local methods produce results with
a more limited range, which can therefore be misleading.

% It's a trade-off: think of it as looking at just the mean vs. looking at just one point

In the following we focus on local explainability methods, relying on the assumption that aggregating many local explanations can help us grasp the model's general behaviour.

\subsection{Local explainability}

Having discarded all global methods, we can still subdivide local methods into several types of explanation. Some of them are outlined in \cite{bellePrinciples2021}:

\paragraph{Local approximation approaches}

One strategy is to transform the model into a simpler, more interpretable one such that the proxy model has the same local behaviour. In LIME \cite{ribeiroWhy2016}, a commonly used method, the proxy is a linear model.
Interestingly, it has recently been shown that many pre-existing local explanation methods, including LIME, can in fact be formulated as instances of the same framework called Local Function Approximation \cite{hanWhich2022}.

\paragraph{Rule-based approaches}

A special case of local approximation approaches, rule-based techniques aim to approximate the model's behaviour by simple logical rules on the input features that hold \emph{locally} (i.e. for a high proportion of other input points in the neighborhood of the input of interest).
One rule-based approach, called \method{Anchors}, does so by reformulating the problem as a multi-armed bandit problem \cite{ribeiroAnchors2018}.
The advantage of rules as explanations is their intelligibility; it has been shown that rules depicting why a model behaved a certain way tend to be well understood compared to other types of explanation \cite{limWhy2009}.

% CF is dual to rule-based \cite{gengComputing2022}.

\paragraph{Counterfactual approaches}

A counterfactual explanation (CFX hereafter) is one or more \emph{virtual points} that are close to the explained input, but for which \emph{the model prediction is different}.
Such an explanation aims to answer the question: ``What would have to change in the input for the model to change its prediction?''
This is reminiscent of adversarial attacks, which consist in finding small perturbations that ``trick'' the model into making a nonsensical prediction \cite{szegedyIntriguing2014,moosavi-dezfooliUniversal2017}.
The difference between an adversarial perturbation and a CFX is not formally agreed upon, however some authors consider that while adversarial perturbations can be imperceptible, CFX should be sparse yet easily visible perturbations \cite{laugelLocal2020}. The argument could also be made that CFX also have a realism constraint \citenote, although that can be the case for adversarial perturbations too: if the perturbation is imperceptible, then the perturbed input is realistic since it is close to a real input point.

CFX provide value in several ways:
\begin{itemize}
    \item They indirectly show which features are being used by the model in its decisions;
    \item They provide insight on the model's understanding of what makes an input point realistic;
    \item They constitute a way to provide \emph{recourse} when the model is used in a setting where recourse is required or desirable (for example in the context of a loan application).
\end{itemize}

The subject of this work will be this last kind of local explanation.

\subsection{Counterfactual explanations}

One of the first and most popular sources on CFX is \cite{wachterCounterfactual2017}. The problem is formulated as an optimization problem constrained by the distance between the explained input $x$ and the candidate counterfactual $\CF{x}$:
\begin{equation}
    \arg\min_{\CF{x}} \max_\lambda \{ \lambda (f(\CF{x}) - y_\target)^2 + d(x, \CF{x}) \}
\end{equation}
where $f$ is the model of interest, $y_\target$ is the target model output (different from the factual output $y = f(x)$) and $d$ is the aformentioned distance metric, which depends on the use-case.

The authors view CFX as an adequate solution to the ``right to explanation'' requirements set by GDPR. Furthermore, in certain contexts such as loan applications, a CFX could constitute a useful piece of advice that can be taken advantage of in a future trial. \citenote{}

\end{document}