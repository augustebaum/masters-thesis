\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}

\begin{document}

\chapter{Conclusion}
\label{ch:conclusion}

In this work, we studied counterfactual explanation methods for tabular data.
We gave an overview of explainable AI, distinguished different methods and their scope, settling on CFX for their ease of comprehension and ability to provide recourse.
Then, we reviewed the literature on CFX and found many methods based on performing gradient descent in a latent space learned through a generative model such as a variational autoencoder or a normalizing flow, which help keep the CFX search within the data manifold and thus guarantee that the CFX are realistic and useful.
Each work gave different criteria for the quality of an explanation, which shows that progress can still be made in this regard.

We drew upon one of these methods, \ls{}, as a simple and fast method that showed promise on image data, in particular
by displaying the progression from a prediction to its counterfactual, something that is seldom shown in the literature
despite the significant amount of iterative methods.
We expanded on it in several key ways:
\begin{enumerate}
    \item We applied it to tabular data while the original \ls{} paper focused on image data, where explanation quality is easier to define.
    \item We implemented it using NICE---an architecture for normalizing flows---as the generative model, to ensure trustworthiness in the produced explanation paths, whereas a basic autoencoder was used in the original \ls{} paper.
    \item We applied it to multi-class classification problems where the original work focused on binary classification. In doing so, we introduced and analyzed different candidates for the right loss function to minimize to achieve high validity.
    \item In an effort to compete with the usual gradient descent methods, such as \revise{} \cite{joshiRealistic2019}, we formulated the path regularization procedure, to embed interpretability criteria within the latent space, whereas the authors of \cite{cohenGifsplanation2022} explicitly refrained from tailoring the latent representation to the task.
    We achieved modest results with our experiment settings, but we are confident that this procedure is theoretically sound.
    \item We probed the path regularization procedure to test its boundaries, in an effort to show that it can be misused to produce nonsensical explanations.
\end{enumerate}

In future work, we would consider attempting our analyses with more sensible experimental parameters
and exploring the landscape for more appropriate latent representations.

\end{document}