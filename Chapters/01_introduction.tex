\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}

\begin{document}


\chapter{Introduction}
\label{ch:introduction}

\section{Context}

\subsection{Explainable AI}

The term "Explainable AI" refers to the endeavor of extracting human-friendly \emph{reasoning} from an AI system, either by a post hoc procedure or by guiding its learning process to give it the ability to produce said reasoning \cite{zhangSurvey2021}.

Some of the reasons driving research in explainable AI (referred to as XAI hereafter) are summarized in \cite{zhangSurvey2021}:
\begin{itemize}
    \item Scientists that use AI hope to recover human-understandable information pertaining to their problem of interest (e.g. in drug design or physics);
    \item Regulators hope to make AI-based decisions accountable, especially those that affect data subjects directly (e.g. in insurance, banking, or in criminal justice);
    \item AI practitioners hope to limit unexpected failure scenarios in their work, especially in cases where failure can be costly (e.g. in medicine or finance).
\end{itemize}

% Note that it has also been argued that in many cases (including in the tabular setting) trying to "explain" black-box models is less productive than using more interpretable-by-design techniques \cite{rudinWhy2019}.

Because of how ubiquitous AI is becoming, XAI research is also blossoming and the research landscape is vast.
Accordingly, some taxonomies have emerged recently that help differentiate methods \cite{zhangSurvey2021, bellePrinciples2021}.
In particular, as mentioned, there exist methods that alter the training process of the model of interest to make it more interpretable; these are called \emph{embedded} methods.
On the other hand, many methods are designed to be applied to pre-trained models in a frozen state -- the models can only be probed, not modified. These methods are referred to as \textsl{post hoc} (literally ``after the fact'').
In this work we focus on the more general-purpose \textsl{post hoc} methods.

One other key characteristic of an XAI method is its \emph{scope}: a \emph{global} XAI method applies to a \emph{whole} fully trained ML model, while a \emph{local} method gives an explanation of \emph{one prediction} by the model.
Global methods produce compact results, but they report general information that does not necessarily reflect
how the model acts in all situations, while local methods produce results with
a more limited range, which are more precise but can lead to misleading interpretations if applied to far-removed situations.

% It's a trade-off: think of it as looking at just the mean vs. looking at just one point

In the following we focus on local explainability methods, relying on the assumption that aggregating many local explanations can help us grasp the model's general behavior.

\subsection{Local explainability}

Setting aside global methods, we can still subdivide local methods into several types of explanation.
Some of them are outlined in \cite{bellePrinciples2021}:

\subsubsection{Local feature attribution approaches}

A popular format for explanations is \emph{feature attributions}: ratings of importance of each input feature, which give an answer to the question ``Which features matter to the model for its prediction?''.
Broadly speaking, there are two types of methods outputting feature attributions: perturbation-based methods and gradient-based methods.

\paragraph{Perturbation-based methods}

One strategy is to find an approximation of the model: a simpler, more interpretable proxy model which has the same local behavior. In \method{LIME}, a commonly used method, the proxy is a linear model \cite{ribeiroWhy2016}.
With the proxy model, it is often straightforward to extract information such as feature attribution: in the case of a linear model, the weight of each feature in the prediction is related to the magnitude of the regression coefficients.


However, finding a proxy model can take a long time, and the proxy would only hold for one point of interest.
Similarly, finding the approximation requires access to at least some training data points, which means that part of the dataset needs to be available on the same machine where the proxy model is generated -- which increases memory consumption and constitutes a potential privacy issue.
Worse, it has been shown that \method{LIME} and other methods can be fooled into proposing explanations that do not accurately reflect the model behavior \cite{slackFooling2020}.

\paragraph{Gradient-based methods}

Another strategy involves computing the gradient of the classification function with respect to the input features: indeed, by definition of the gradient, this precisely measures how sensitive the prediction is to a change in one of the input features.
One of the most frequently used methods of this kind is \method{IntegratedGradients} \cite{sundararajanAxiomatic2017}, which answers the question: ``How relevant to the prediction is feature $i$ of the point of interest $x$ compared with some other reference point $x'$?''.

Notice that the explanation is only given with respect to the reference point $x'$, so choosing a different $x'$ can yield a different explanation; furthermore, the choice of $x'$ is not always obvious.

Interestingly, it has recently been shown that many pre-existing local explanation methods, including \method{LIME} and \method{IntegratedGradients}, can in fact be formulated as instances of the same framework called Local Function Approximation \cite{hanWhich2022}.

\subsubsection{Rule-based approaches}

Rule-based techniques aim to approximate the model's behavior by simple logical rules on the input features that hold locally (\ie{} for a high proportion of other input points in the neighborhood of the input of interest).
As such, they can be understood as a special case of local approximation approaches.
One rule-based approach, called \method{Anchors}, does this by reformulating the problem as a multi-armed bandit problem \cite{ribeiroAnchors2018}.
The advantage of rules as explanations is their intelligibility: it has been shown that rules depicting why a model behaved a certain way tend to be well understood compared to other types of explanation \cite{limWhy2009}.

\subsubsection{Counterfactual approaches}\label{par:cfx}

A counterfactual explanation (CFX hereafter) is one or more \emph{virtual points} that are close to the explained input, but for which \emph{the model prediction is different}.
Such an explanation aims to answer the question: ``What would have to change in the input for the model to change its prediction?''
This is reminiscent of adversarial attacks, which consist in finding small perturbations that ``trick'' the model into making a nonsensical prediction \cite{szegedyIntriguing2014,moosavi-dezfooliUniversal2017}.
The differences between an adversarial perturbation and a CFX are subject to debate, and recently efforts have been made to reconcile the two research areas \cite{freieslebenIntriguing2022}.
Some authors consider that while adversarial perturbations can be imperceptible, CFX should be sparse yet easily visible perturbations \cite{laugelLocal2020}.
The argument could also be made that CFX also have a realism constraint \cite{vermaCounterfactual2020}, although that can be the case for adversarial perturbations too: if the perturbation is imperceptible, then the perturbed input is realistic since it is close to a real input point.

CFX provide value in two keys ways \cite{samekExplainable2019}:
\begin{itemize}
    \item they indirectly show which features are being used by the model in its decisions, and
    \item they constitute a way to provide \emph{recourse} when the model is used in a setting where recourse is required or desirable (for example in the context of a loan application).
\end{itemize}

The subject of this work will be this last kind of local explanation, and we will describe it further in the next section.

\section{Counterfactual explanations}

\subsection{A first example: \citeauthor{wachterCounterfactual2017}}
\label{intro/wachter}

One of the first and most frequently cited sources on CFX is \citeauthor{wachterCounterfactual2017} \cite{wachterCounterfactual2017}. In this work, the problem is formulated as an optimization problem constrained by the distance between the explained input $x$ and a candidate counterfactual $x'$:
\begin{equation}
\arg\min_{\CF{x}} \min_\lambda \{ \lambda (f(\CF{x}) - y_\target)^2 + d(x, \CF{x}) \}
\arg\min_{x'} \min_\lambda \{ \lambda (f(x') - y')^2 + d(x, x') \}
\end{equation}
where $f$ is the model of interest, $y'$ is the target model output
(different from the factual output $y = f(x)$) and $d$ is the aforementioned
distance metric, which depends on the use-case.

The authors view CFX as an adequate solution to the ``right to explanation'' requirements set by the GDPR \cite{kaminskiRight2018}.
Furthermore, in certain contexts such as loan applications, a CFX could constitute a useful piece of advice that can be taken advantage of in a future trial: the candidate that sees their application denied can tell what they should do to improve their chances.

Yet, while CFX seem helpful in improving trust in black-box machine learning models, further thought brings to light difficult problems that threaten the trustworthiness of the explanations \emph{themselves}.

\subsection{Formalizing counterfactual explanations}

% There are many kinds of explanation, each with their own specific formalism issues. Here are some of those for CFX.
As we saw when describing local explainability techniques, there are several competing forms of local explanation.
The field of XAI in general has suffered from a crisis of formalism \cite{liptonMythos2017, leavittFalsifiable2020}.
In particular CFX have a unique set of issues relating to rating explanations, that set it apart from its siblings.

\subsubsection{The concurrent goals of counterfactual explanations}

As noted by the authors of \cite{wachterCounterfactual2017}, one appeal of CFX is their ability to provide a large quantity of information in a seemingly compact form: a CFX can serve as a local feature attribution rating, highlighting which features the model used to differentiate between classes in a specific prediction, and it can also enable stakeholders to offer recourse when this is appropriate.
However, these two coupled objectives can sometimes conflict and make CFX appear less trustworthy as a result.

For example, consider the problem of allowing protected attributes to be modified.
In a hypothetical loan application setting, consider a model that takes a set of personal information about a candidate and predicts if they are likely to default on their loan.
In particular, suppose some protected attribute is being used in the prediction, such as ethnicity or gender.
If a CFX system were used for recourse, it might be sensible to set it up to completely ignore such protected attributes: could we expect a person to change their gender in order to increase their chances of receiving a loan?
On the other hand, say a CFX system were used for testing the model's learning. If the protected attribute is frozen in place, then by definition it will never being identified as useful for the model's decision, \emph{even if the model actually makes use of it}.
In other words, if a CFX displays that the protected attribute is unchanged, in the absence of other information, it is not possible to tell if the model is unbiased, or if the CFX system is unbiased.

\subsubsection{The out-of-distribution problem}

One major drawback with CFX is that, since they generate virtual points, there is a risk that these new points look realistic, yet quite different from the data used to train the model.
In this case the model could generalize poorly and the explanation could therefore be misleading.
Worse, the new points could look completely unrealistic (e.g. containing logical inconsistencies, such as a person having a negative age), in which case the explanation might be difficult or impossible to interpret.

Determining whether a given point in the input space is ``realistic'' is a problem in itself, and is studied through various lenses from different disciplines of data analysis \cite{yangGeneralized2022}.
In the following we will refer to this as the ``out-of-distribution'' problem.

\subsubsection{Semantic explanations}
\label{intro/semanticity_problem}

% We have already discussed the similarity between CFX and adversarial perturbations in \autoref{par:cfx}. 

% Where adversarial perturbations usually aim to be imperceptible to humans (but highly noticeable to the model), CFX are designed to be recognized as 

An aim of CFX would be that, if the model has recovered some meaningful information about the data, then the explanation should reveal it. That is to say, we would like to trust that, if the provided explanations are gibberish, then this must mean that what the model learned is not meaningful to a human, at a glance.
Of course, this raises a highly philosophical issue: what is meaning? 
% Surely, we cannot recognize interesting behaviour within explanations unless we know about it beforehand
If a concept means nothing to you, then by definition you could easily miss it and move on.
Therefore, if the explanation presents us with what seems to be gibberish, it is not possible to distinguish gibberish that actually constitutes interesting concepts from random noise that truly contains no information.

\subsection{On the differences between image and tabular data}
\label{intro/image_data}

In the field of CFX, as everywhere in deep learning, the problem domain is an important consideration.
In particular, the question of realism of the generated points has been approached differently depending on whether the data is in tabular form or in image form.
Indeed, some advances have been made on metrics for realism of generated images: for example, the Fr√©chet inception distance (or FID) is the current state-of-the-art for metrics of image quality \cite{heuselGANs2017}.

% Things are harder in tabular data because tabular data are not cleanly reduced to lower-dimension spaces the way images can.

Additionally, when discussing the effectiveness of deep learning for image problems, one argument that is often invoked is the manifold hypothesis \cite{bengioRepresentation2013}.
This states that for many tasks involving real-life high-dimensional data, the data generally do not span across the whole input space $\inputspace$; rather, they lie close to a low-dimensional manifold contained within it.
In particular, images have intrinsic structure to them, for example it is often the case that rotating an image does not change the meaning it carries.
This structure can be encoded within a neural network, \eg{} with convolutional layers \cite{zhangShiftinvariant1988, lecunBackpropagation1989}, and doing this tends to achieve high success rates.
Recently this idea of encoding the problem structure within a neural network architecture has been discussed in detail in \cite{bronsteinGeometric2021}.
Note that the manifold hypothesis was recently put back into question, with the authors of \cite{brownUnion2022} instead proposing that the data lie on a union of manifolds.

By contrast, no such hypothesis has been made for tabular data, where feature engineering is a much more important factor in model performance.
Accordingly, the state-of-the-art on tabular data is methods based on decision trees \cite{shwartz-zivTabular2022}.

Now that we have presented the overall problem and its stakes, let us detail the technical background needed for our analyses.

\end{document}