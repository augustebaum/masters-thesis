\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}

\begin{document}

\chapter{Technical background}
\label{ch:background}

In this chapter we present some technical notions that will be relevant in our analysis, and we present our notation.

For a vector $x$ in $\R^N$ we write $x_i$ for the $i$\nth component of $x$; if the vector is written with a subscript already, \eg{} $y_\text{true}$, we can also denote the $i$\nth{} component by $y^{(i)}_\text{true}$.
We abuse this subscript notation in specific cases:
when $c \in \setclasses$ we use $y_c$ to denote a label vector as would be used for training a machine learning model: $y_c = (\delta_{jc})_{j=1}^{\outputdim}$ where $\delta$ refers to the Kronecker delta.

The $L_p$ norm is a measure of magnitude and is usually written $\norm{\cdot}_p$.
It is computed as $\norm{x}_p = \sqrt[p]{\sum_i \abs{x_i}^p}$.

\section{Classifier}

Consider the problem of detecting a cat in a grayscale image. This problem can be formalized as follows.

Let there be an \emph{input space} $\inputspace$ with dimensionality $\inputdim$, typically $\R^\inputdim$ in our analysis. For cat detection $\inputspace = \{\text{image with $\inputdim$ pixels}\}$.

Let there be a set of classes $\setclasses$ with $C$ classes. For cat detection $\setclasses = \{\text{cat}, \text{no cat}\}$.

A \emph{classifier} is a function $f: \R^\inputdim \to \R^\outputdim$. In our analysis we assume $f$ is a deep neural network with trainable parameters $w$, and that it is differentiable. The networks we train are typically multi-layer perceptrons, or MLP for short.
Usually the outputs of the classifier are referred to as \emph{logits}, and they are further processed to obtain a probability mass using the $\softmax$ function:
\begin{equation}
    \softmax(x) = (\softmax_i(x))_{i=1}^{\outputdim} \text{ where }
    \softmax_i(x) = \frac{\exp x_i}{\sum_{j=1}^{\outputdim} \exp x_j}
\end{equation}

Finding the optimal parameters of a deep learning model is usually done with an iterative optimization algorithm that minimizes a loss function $\loss$, typically \emph{gradient descent}.
Gradient descent consists in computing the gradient of $\loss$ with respect to the model parameters, written $\nabla_w \loss$ or $\grad{\loss}{w}$, which gives the ``direction of steepest ascent'' of $\loss$.
Then, the model parameters are adjusted slightly in the opposite direction of the gradient (to make $\loss$ decrease in subsequent training steps): $w_{t+1} = w_t - \nabla_w \loss$.

The loss we use to train our classifiers in this work is the \emph{cross-entropy} $\ce$.
Given logits output by $f$ and the true class label $c$, the cross-entropy is computed as:
\begin{equation}
\ce(f(x), c) = - \log \softmax_c(f(x))
\end{equation}

When training, we separate the full dataset $\data$ into a training set, a validation set and a test set, denoted by $\trainset$, $\valset$ and $\testset$ respectively.
The optimization algorithm is run on data from $\trainset$, and after every epoch we compute the validation error by running the model on $\valset$. Finally, we test how well our model generalizes to unseen examples by running it on $\testset$.

A multi-layer perceptron, or MLP for short, is a classical example of a deep learning model architecture.
It consists in several layers containing a linear transformation, followed by a nonlinear \emph{activation function}.
One of these is the \emph{rectified linear unit}, usually referred to as $\relu$, and defined as $\relu(x) = \max\{0, x\}$.

\section{Autoencoder}

A generative model is a function $g$ that can generate points in $\inputspace$ that look similar to points in a dataset; finding such a model is equivalent to finding the distribution of points in the dataset, denoted by $P(x)$.

In particular, a latent variable generative model is a mapping $\dec: \latentspace \to \inputspace$ that allows modeling the data distribution $P(x)$ by assuming a given $x \in \inputspace$ has been generated by drawing a latent variable $z$ from a known distribution with support in $\latentspace$ and then setting $x = \dec(z)$.
Intuitively, drawing from a tutorial by \citeauthor{doerschTutorial2021} \cite{doerschTutorial2021}, consider the problem of generating realistic handwritten digits.
Here, one could sample a digit from 0 to 9 randomly (following, say, a uniform distribution) and then relying on $\dec$ to produce an image that looks like that digit, in handwritten form. In this case, the digit would be the latent variable $z$.

A subclass of generative models is that of autoencoders.
An autoencoder is a pair of functions, commonly referred to as the ``encoder'' and ``decoder'' and denoted by $(\enc: \inputspace \to \latentspace, \dec: \latentspace \to \inputspace)$ implicitly defining a latent space $\latentspace$, with the constraint that the reconstruction error $\norm{x - \dec(\enc(x))}$ is as small as possible.
It is common for us to refer to points in latent space as ``latents''.
In our work we make use of autoencoder models that are based on deep neural networks and are thus differentiable.
In addition, for our purposes it is not sufficient that the reconstruction error be minimized: we require continuity of the decoder, so that mapping a line in $\latentspace$ through $\dec$ results in a continuous path in $\inputspace$.

\subsection{Variational autoencoder}

\citenote{review definition of vae}
% A variational autoencoder (or VAE for short) \cite{kingmaAutoEncoding2014} is an autoencoder model in which we learn a distribution $Q(z | x)$ that corresponds to the likelihood of a $z$ giving rise to a particular $x$.
% This distribution is assumed to be a standard Gaussian parametrized by $x$: $Q(z | x) = \normal(\mu(x), \Sigma(x))$ where $\mu(x)$ and $\Sigma(x)$ are learnable networks that output respectively the mean and the covariance matrix of the Gaussian. We then enforce this assumption by minimizing the KL-divergence $\kl(\normal(\mu(x), \Sigma(x)) \| \normal(0, I))$
% The encoder $\enc$ is then given by $\enc(x) = \mu(x)$.

For a more detailed introduction to VAEs we highly recommend the tutorial by \citeauthor{doerschTutorial2021} \cite{doerschTutorial2021}.

\subsection{Flow-based models}
\label{bg/nf}

Another type of generative model is the normalizing flow (or NF for short), in which we assume again a latent variable $z$ that is typically standard Gaussian-distributed and that leads to an $x$ via a transformation $g$ (our $\dec$ function).
Usually, this $g$ is decomposed into several small perturbations $g_i$ so that $x = g(z) = (g_K \after g_{K-1} \after \ldots \after g_1) (z)$.
Importantly, each $g_i$ is \emph{invertible}, hence $g$ itself is invertible and one can easily express and therefore maximize the log-likelihood $\log P(x)$ using the change-of-variable formula:
%
\begin{align*}
    P(x)               & = P(z) \abs{ \det \diff{g}{z} }       \\
                       & = P(z) \abs{ \det \left(
        \diff{g_K(z_{K-1})}{z_{K-1}}
        \diff{g_{K-1}(z_{K-2})}{z_{K-2}}
        \ldots
        \diff{g_1(z)}{z}
    \right)}                                                   \\
                       & = P(z) \abs{
        \det \left(
        \diff{g_K(z_{K-1})}{z_{K-1}}
        \right)
        \det \left(
        \diff{g_{K-1}(z_{K-2})}{z_{K-2}}
        \right)
        \ldots
        \det \left(
        \diff{g_1(z)}{z}
        \right)
    }                                                          \\
                       & = P(z) \abs{
        \det \left(
        \diff{g_K(z_{K-1})}{z_{K-1}}
        \right)
    }
    \abs{
        \det \left(
        \diff{g_{K-1}(z_{K-2})}{z_{K-2}}
        \right)
    }
    \ldots
    \abs{
        \det \left(
        \diff{g_1(z)}{z}
        \right)
    }                                                          \\
    \implies \log P(x) & = \log P(z) + \sum_{i=1}^K \log \abs{
        \det \diff{g_i(z_{i-1})}{z_{i-1}}
    }\quad \text{(defining $z_0 = z$)}
\end{align*}
where $\det$ denotes the determinant and $\diff{g}{z}$ the Jacobian matrix of $g$.
We can then train the model using the negative log-likelihood (or NLL for short)

Unlike VAEs, the transformation $g$ is guaranteed to be invertible, but this requires that the dimensionality of $\latentspace$ be at least $\inputdim$.

\subsubsection{Nonlinear independent components estimation}

This implementation of a NF first presented in \cite{dinhNICE2015}, hereafter referred to as NICE, allows crafting flows $g_i$ that are invertible, even if they contain highly nonlinear components.
The $g_i$ are referred to as \emph{coupling layers}.

This is achieved by separating $z_i$ into two parts: one left unmodified and the other put through some nonlinear layer with learnable parameters.
Then, the two parts are passed to a coupling layer that should be invertible with respect to its first argument given the second.
For example, in an \emph{additive coupling layer} the coupling layer is an addition: $g_i(a; b) = a + b$, which is indeed invertible with respect to $a$ given $b$: namely, its inverse is $g_i^{-1}(a; b) = a - b$.
Thus, by construction, the transformation is invertible regardless of the nonlinearity: in \autoref{eq:coupling_layer} we show the relation between layer $i$ and layer $i+1$, with the linearity in this layer denoted by $h_{i+1}$. $z_i$ is divided into parts $A$ and $B$, and it is possible to get $z_i$ back entirely from $z_{i+1}$:

\begin{equation}
    \label{eq:coupling_layer}
    \begin{cases}
        z_{i+1}^A = z_i^A \\
        z_{i+1}^B = h_{i+1}(z_i^A) + z_i^B
    \end{cases}
    \Leftrightarrow
    \begin{cases}
        z_i^A = z_{i+1}^A \\
        z_i^B = -h_{i+1}(z_{i+1}^A) + z_{i+1}^B
    \end{cases}
\end{equation}


\end{document}