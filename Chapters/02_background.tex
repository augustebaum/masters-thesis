\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}

\begin{document}

\chapter{Technical background}
\label{ch:background}

Input space $\inputspace$, typically $\R^\inputdim$ in our analysis.

Classifier $f: \R^\inputdim \to \R^\outputdim$ assumed to be differentiable

Trained using cross-entropy $\ce$

Gradient descent?


\section{Autoencoder}

A generative model is a function $g$ that can generate points in $\inputspace$ that look similar to points in a dataset; finding such a model is equivalent to finding the distribution of points in the dataset, denoted by $P(x)$.

A latent variable generative model is a mapping $\dec: \latentspace \to \inputspace$ that allows modeling the data distribution $P(x)$ by assuming a given $x \in \inputspace$ has been generated by drawing a latent variable $z$ from a known distribution with support in $\latentspace$ and then setting $x = \dec(z)$.
Intuitively, drawing from a tutorial by \citeauthor{doerschTutorial2021}, consider the problem of generating realistic handwritten digits.
Here, one could sample a digit from 0 to 9 randomly (following, say, a uniform distribution) and then relying on $\dec$ to produce an image that looks like that digit, in handwritten form. In this case, the digit would be the latent variable $z$.

A subclass of generative models is that of autoencoders.
An autoencoder is a pair of functions, commonly referred to as the ``encoder'' and ``decoder'' and denoted by $(\enc: \inputspace \to \latentspace, \dec: \latentspace \to \inputspace)$ implicitly defining a latent space $\latentspace$, with the constraint that the reconstruction error $\norm{x - \dec(\enc(x))}$ is as small as possible.
In our work we make use of autoencoder models that are based on deep neural networks and are thus differentiable.
In addition, for our purposes it is not sufficient that the reconstruction error be minimized: we require continuity of the decoder, so that mapping a line in $\latentspace$ through $\dec$ results in a continuous path in $\inputspace$.

\subsection{Variational autoencoder}

A variational autoencoder (or VAE for short) is an autoencoder model in which we learn a distribution $Q(z | x)$ that corresponds to the likelihood of a $z$ giving rise to a particular $x$.
This distribution is assumed to be a standard Gaussian and we enforce this by minimizing the KL-divergence $\kl(\normal(\mu(x), \Sigma(x)) \| \normal(0, I))$ where $\mu(x)$ and $\Sigma(x)$ are learnable networks that take $x$ as input and output respectively the mean and the covariance matrix of the Gaussian.
The encoder $\enc$ is then given by $\enc(x) = \mu(x)$.

For a more detailed introduction to VAEs we highly recommend the tutorial by \citeauthor{doerschTutorial2021} \cite{doerschTutorial2021}.

\subsection{Flow-based models}

Another type of generative model is the normalizing flow, in which we assume again a latent variable $z$ that is typically standard Gaussian-distributed and that leads to an $x$ via a transformation $g$ (our $\dec$ function).
Usually, this $g$ is decomposed into several small perturbations $g_i$ so that $x = g(z) = (g_K \after g_{K-1} \after \ldots \after g_1) (z)$.
Importantly, each $g_i$ is \emph{invertible}, hence $g$ itself is invertible and one can easily express and therefore maximize the log-likelihood $\log P(x)$ using the change-of-variable formula:
%
\begin{align*}
    P(x)               & = P(z) \abs{ \det \diff{g}{z} }       \\
                       & = P(z) \abs{ \det \left(
        \diff{g_K(z_{K-1})}{z_{K-1}}
        \diff{g_{K-1}(z_{K-2})}{z_{K-2}}
        \ldots
        \diff{g_1(z)}{z}
    \right)}                                                   \\
                       & = P(z) \abs{
        \det \left(
        \diff{g_K(z_{K-1})}{z_{K-1}}
        \right)
        \det \left(
        \diff{g_{K-1}(z_{K-2})}{z_{K-2}}
        \right)
        \ldots
        \det \left(
        \diff{g_1(z)}{z}
        \right)
    }                                                          \\
                       & = P(z) \abs{
        \det \left(
        \diff{g_K(z_{K-1})}{z_{K-1}}
        \right)
    }
    \abs{
        \det \left(
        \diff{g_{K-1}(z_{K-2})}{z_{K-2}}
        \right)
    }
    \ldots
    \abs{
        \det \left(
        \diff{g_1(z)}{z}
        \right)
    }                                                          \\
    \implies \log P(x) & = \log P(z) + \sum_{i=1}^K \log \abs{
        \det \diff{g_i(z_{i-1})}{z_{i-1}}
    }\quad \text{(defining $z_0 = z$)}
\end{align*}
where $\det$ denotes the determinant and $\diff{g}{z}$ the Jacobian matrix of $g$.

\subsubsection{Nonlinear independent components estimation}
This implementation of a NF, hereafter referred to as NICE, ???

\end{document}