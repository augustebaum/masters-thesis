\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}

\begin{document}

\chapter{Technical background}
\label{ch:background}

In this chapter we present some technical notions that will be relevant in our analysis, and introduce some notation.

\section{Preliminaries}

For a vector $x$ in $\R^N$ we write $x_i$ for the $i$\nth component of $x$; if the vector is written with a subscript already, \eg{} $y_\text{true}$, we can also denote the $i$\nth{} component by $y^{(i)}_\text{true}$.

The Kronecker delta function is defined as
\begin{align*}
\delta_{x, x'} = \begin{cases}
    1 & \text{if } x = x' \\
    0 & \text{otherwise} \\
\end{cases}
\end{align*}

$y_i \in \R^N$ can also refer to an $N$-component vector that is 1 in position $i$ and 0 everywhere else, so $y_i = (\delta_{ij})_{j=1}^N$.

A norm is a measure of magnitude; there are many existing norms, so when this is undetermined the norm of a vector $x$ is often written $\norm{x}$.
The $L_p$ norm is an example of a norm and is usually written $\norm{\cdot}_p$.
It is computed as $\norm{x}_p = \sqrt[p]{\sum_i \abs{x_i}^p}$ where $x_i$ is the $i$\nth component of $x$.

Let $F(x_1, \ldots, x_n)$ be a function of $n$ variables outputting a scalar, differentiable with respect to each variable.
In multivariable calculus, the \emph{partial derivative} of $F$ with respect to $x_i$ generalizes the usual single-variable derivative
and is written $\del{F}{x_i}$.

The \emph{gradient} of $F$ is the vector
\begin{align*}
    \grad{F}{x} =  \begin{pmatrix}
        \del{F}{x_1} & \cdots  & \del{F}{x_n}
    \end{pmatrix}
\end{align*}

When $F$ is vector-valued with output components $F_1, \ldots, F_p$, the equivalent of the gradient is the \emph{Jacobian matrix}:
\begin{align*}
    \jac{F}{x} =
    \begin{pmatrix}
        \del{F_1}{x_1} & \cdots  & \del{F_1}{x_n} \\
        \vdots         & \ddots  & \vdots         \\
        \del{F_p}{x_1} & \cdots  & \del{F_p}{x_n}
    \end{pmatrix}
\end{align*}

\section{Classifier}

Consider the problem of detecting an object in a grayscale image. This problem can be formalized as follows.

Let there be an \emph{input space} $\inputspace$ with dimensionality $\inputdim$, typically $\R^\inputdim$ in our analysis.
For the object detection task, $\inputspace = \{\text{image with $\inputdim$ pixels}\}$.

Let there be a set of classes $\setclasses$ containing $C$ classes. For object detection $\setclasses = \{\text{object}, \text{no object}\}$ or $\{0, 1\}$.

A \emph{classifier} is a function $f: \R^\inputdim \to \R^\outputdim$.
Usually the outputs of the classifier are referred to as \emph{logits}, written $u \in \R^\outputdim$,
and they are further processed to obtain a probability mass using the $\softmax$ function:
\begin{equation}
    \softmax(u) = (\softmax_i(u))_{i=1}^{\outputdim} \text{ where }
    \softmax_i(u) = \frac{\exp u_i}{\sum_{j=1}^{\outputdim} \exp u_j}
\end{equation}
We use the notations $\softmax_i(x)$ and $\softmax(x)_i$ interchangeably.
When it is unambiguous we also write $p_i = \softmax_i(x)$ for brevity.

A classification task consists in finding a classifier that fits real-life data as best as possible:
given many examples of images $x \in \inputspace$ labelled as $y_\text{true} \in \{0, 1\}$ depending on if the object
of interest is present in $x$, find $f$ such that
$\loss(f(x), y_\text{true})$ is small for all $x$, where $\loss$ is some distance metric, often called a \emph{loss}.

For example, when the training example is $(x, \text{no object})$ we encode it as $(x, 0)$ and the ideal probability mass $p$
would be $(1, 0)$ (a 1 in position 0 and zeros elsewhere);
when the training example is $(x', \text{object})$ we encode it as $(x', 1)$ and then the ideal $p$ would be $(0, 1)$ (a 1
in position 1 and zeros elsewhere).

In our analysis, as in deep learning in general, we typically let $f = f_w$ be a deep learning model with trainable parameters $w$.
Therefore, the classification task is reduced to finding the set of parameters $w$ that minimizes
the loss $\loss$.

Finding the optimal $w$ is usually done with an iterative optimization algorithm that minimizes $\loss$, typically \emph{gradient descent}.
Gradient descent consists in computing the gradient of $\loss$ with respect to $w$, $\grad{\loss}{w}$, which gives the ``direction of steepest ascent'' of $\loss$.
Then, the model parameters are adjusted slightly in the \emph{opposite direction} of the gradient (to make $\loss$ decrease in subsequent training steps): $w_{t+1} = w_t - \nabla_w \loss$.
The loss we use to train our classifiers in this work is the \emph{cross-entropy} $\ce$.
Given logits output by $f$ and the true class label $c$, the cross-entropy is computed as:
\begin{equation}
\ce(f(x), c) = - \log \softmax_c(f(x))
\end{equation}

A \emph{multi-layer perceptron}, or MLP for short, is a classic example of a deep learning model architecture.
It consists in several \emph{layers}; in its most basic form, a layer consists in a linear transformation,
followed by a nonlinear \emph{activation function}.
One of these activation functions is the \emph{rectified linear unit}, usually abbreviated to $\relu$, and defined as $\relu(x) = \max\{0, x\}$.

When optimizing the $w$ (training the model), we do not iterate on the full dataset $\data$.
We first separate it into a training set, a validation set and a test set, denoted by $\trainset$, $\valset$ and $\testset$ respectively.
The optimization algorithm is run on the train set repeatedly;
an \emph{epoch} corresponds to one full on the whole train set, and after every epoch we compute the validation error by running the model on $\valset$.
When this validation error stops decreasing, this is a sign that the training process can be stopped.
Finally, we test how well our model generalizes to unseen examples by running it on $\testset$.

\section{Autoencoder}

A \emph{generative model} is a function $g$ that can generate points in $\inputspace$ that look similar to points in a dataset; finding such a model is equivalent to finding the distribution of points in the dataset, denoted by $P(x)$.

In particular, a \emph{latent variable generative model} is a mapping $\dec: \latentspace \to \inputspace$ that allows modeling the data distribution $P(x)$, which is an approximation
of the true distribution from which the data originate.
To achieve this, we assume that a given $x \in \inputspace$ has been generated by drawing a latent variable $z$ from a known distribution with support in $\latentspace$ and then setting $x = \dec(z)$.
$\latentspace$ is called the \emph{latent space} and its dimensionality is written $\latentdim$.

In simple terms, consider the problem of generating realistic handwritten digits.
In order to generate such an image, one could start by sampling a digit $z$ from 0 to 9 uniformly and then
run $\dec(z)$ to produce an image that looks like that digit, in handwritten form.
This example is adapted from a tutorial by \citeauthor{doerschTutorial2021} \cite{doerschTutorial2021},

A subclass of generative models is that of \emph{autoencoders}.
An autoencoder is a pair of functions, commonly referred to as the ``encoder'' and ``decoder'' and denoted by $(\enc: \inputspace \to \latentspace, \dec: \latentspace \to \inputspace)$ implicitly defining a latent space $\latentspace$, with the constraint that the reconstruction error $\norm{x - \dec(\enc(x))}$ be as small as possible.
It is common for us to refer to points in latent space as ``latents''.
In our work we make use of autoencoder models that are based on deep neural networks and, like for classifiers
are learned by optimizing their parameters:
the encoder is written $\enc_\phi$ and the decoder is written $\dec_\psi$; when it is not ambiguous we omit the subscripts.
In addition, for our purposes it is not sufficient that the reconstruction error be minimized: we require continuity of the decoder, so that mapping a straight line in $\latentspace$ through $\dec$ results in a continuous path in $\inputspace$.

\subsection{Variational autoencoder}

One way to meet this continuity requirement is with a \emph{variational autoencoder} (or VAE for short) \cite{kingmaAutoEncoding2014}.
This is an autoencoder model in which we learn a distribution $Q(z | x)$ that corresponds to the likelihood of a $z$ giving rise to a particular $x$.
This distribution is assumed to be a standard Gaussian parametrized by $x$: $Q(z | x) = \normal(\mu(x), \Sigma(x))$ where $\mu(x)$ and $\Sigma(x)$ are learnable networks that output respectively the mean and the covariance matrix of the Gaussian.
We enforce this assumption by minimizing the KL-divergence $\kl(\normal(\mu(x), \Sigma(x)) \| \normal(0, I))$.
The encoder $\enc$ is then given by $\enc(x) = \mu(x)$.

For a more detailed introduction to VAEs we highly recommend the tutorial by \citeauthor{doerschTutorial2021} \cite{doerschTutorial2021}.

\subsection{Flow-based models}
\label{bg/nf}

Another type of generative model is the \emph{normalizing flow} (or NF for short), in which we assume again a latent variable $z$ that is typically standard Gaussian-distributed and that leads to an $x$ via a transformation $g$ (our $\dec$ function).
Usually, this $g$ is decomposed into several small perturbations $g_i$ so that
\begin{align*}
x = g(z) = (g_K \after g_{K-1} \after \ldots \after g_1) (z).
\end{align*}
where $\after$ denotes composition of functions.
Importantly, each $g_i$ is \emph{invertible}, hence $g$ itself is invertible and one can express and maximize the log-likelihood $\log P(x)$ using the change-of-variable formula:
%
\begin{align*}
    P(x)               & = P(z) \abs{ \det \diff{g}{z} }       \\
                       & = P(z) \abs{ \det \left(
        \diff{g_K(z_{K-1})}{z_{K-1}}
        \diff{g_{K-1}(z_{K-2})}{z_{K-2}}
        \ldots
        \diff{g_1(z)}{z}
    \right)}                                                   \\
                       & = P(z) \abs{
        \det \left(
        \diff{g_K(z_{K-1})}{z_{K-1}}
        \right)
        \det \left(
        \diff{g_{K-1}(z_{K-2})}{z_{K-2}}
        \right)
        \ldots
        \det \left(
        \diff{g_1(z)}{z}
        \right)
    }                                                          \\
                       & = P(z) \abs{
        \det \left(
        \diff{g_K(z_{K-1})}{z_{K-1}}
        \right)
    }
    \abs{
        \det \left(
        \diff{g_{K-1}(z_{K-2})}{z_{K-2}}
        \right)
    }
    \ldots
    \abs{
        \det \left(
        \diff{g_1(z)}{z}
        \right)
    }                                                          \\
    \implies \log P(x) & = \log P(z) + \sum_{i=1}^K \log \abs{
        \det \jac{g_i(z_{i-1})}{z_{i-1}}
    }\quad \text{(defining $z_0 = z$)}
\end{align*}
where $\det$ denotes the determinant of a matrix.
We can then train the model by minimizing the \emph{negative log-likelihood} (or NLL for short).

Unlike in VAEs, the transformation $g$ is guaranteed to be invertible, but this requires that the dimensionality of $\latentspace$ be at least $\inputdim$.

\subsubsection{Nonlinear independent components estimation}

This implementation of a NF first presented in \cite{dinhNICE2015}, hereafter referred to as NICE, allows crafting flows $g_i$ that are invertible, even if they contain highly nonlinear components.
The $g_i$ are referred to as \emph{coupling layers}.

This is achieved by separating $z_i$ into two parts: one left unmodified and the other put through some nonlinear layer with learnable parameters.
Then, the two parts are passed to a coupling layer that should be invertible with respect to its first argument given the second.
For example, for an \emph{additive coupling layer} this is an addition: $g_i(a; b) = a + b$, which is indeed invertible with respect to $a$ given $b$: namely, its inverse is $g_i^{-1}(a; b) = a - b$.
Thus, by construction, the transformation is invertible regardless of the nonlinearity: in \autoref{eq:coupling_layer} we show the relation between layer $i$ and layer $i+1$, with the nonlinearity in this layer denoted by $h_{i+1}$. $z_i$ is divided into parts $A$ and $B$, and it is possible to get $z_i$ back entirely from $z_{i+1}$:

\begin{equation}
    \label{eq:coupling_layer}
    \begin{cases}
        z_{i+1}^A = z_i^A \\
        z_{i+1}^B = h_{i+1}(z_i^A) + z_i^B
    \end{cases}
    \Leftrightarrow
    \begin{cases}
        z_i^A = z_{i+1}^A \\
        z_i^B = -h_{i+1}(z_{i+1}^A) + z_{i+1}^B
    \end{cases}
\end{equation}

\citeauthor{dinhNICE2015} note that this transformation preserves volume (its Jacobian is 1), which can restrict the model's ability to fit the distribution during training, \eg{} if some dimensions are found to be completely unimportant.
To give the model more flexibility, they introduce scaling parameters $S_{ii}$.
For a standard Gaussian prior, with rescaling, the negative log-likelihood ($\nll$) can be derived as follows:
\begin{align*}
    \nll(x)
     & = - \log p_X(x)                                                                               \\
     & = - \log \left( p_Z(z) \prod_{i=1}^\latentdim \abs{S_{ii}} \right)                            \\
     & = - \log p_Z(z) - \log \prod_{i=1}^\latentdim \abs{S_{ii}}                                    \\
     & = - \log p_Z(z) - \sum_{i=1}^\latentdim \log \abs{S_{ii}}                                     \\
     & = - \log p_Z(z) - \sum_{i=1}^\latentdim \log \abs{S_{ii}}                                     \\
     & = - \log \left( \frac{1}{\sqrt{(2 \pi)^\latentdim}} \exp\left(- \frac{z^T z}{2}\right)\right)
    - \sum_{i=1}^\latentdim \log \abs{S_{ii}}                                                        \\
     & = - \log \frac{1}{\sqrt{(2 \pi)^\latentdim}}
    - \log \exp\left(- \frac{z^T z}{2}\right)
    - \sum_{i=1}^\latentdim \log \abs{S_{ii}}                                                        \\
     & = \frac{\latentdim}{2} \log 2 \pi
    +  \frac{z^T z}{2}
    - \sum_{i=1}^\latentdim \log \abs{S_{ii}}.
\end{align*}
where $z = \enc(x)$.

\section{Counterfactual explanations}

Consider a classifier $f$ trained to take an input $x \in \inputspace$ and predict a class $c \in \setclasses$.
We call this predicted class the \emph{source} class and we write it $\source$.
A counterfactual explanation for $x$ is a point $\CF{x}$ that is comparable with $x$ but for which the predicted
class is different.
In some cases it is possible to specify a \emph{target} class that $\CF{x}$ should be predicted as; it is written $\target$.

Equipped with these concepts and notation, we can now give an overview of the literature on counterfactual explanation generation methods making use of generative models.


\end{document}