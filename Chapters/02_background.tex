\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../Figures/}}}

\begin{document}

\chapter{Technical background}
\label{ch:background}

In this chapter we present some technical notions that will be relevant in our analysis, and we present our notation.

Input space $\inputspace$, typically $\R^\inputdim$ in our analysis.

Classifier $f: \R^\inputdim \to \R^\outputdim$ assumed to be differentiable (typically a multi-layer perceptron, or MLP for short).

Trained using cross-entropy $\ce$:
\begin{equation}
    \ce(y_\text{predict}, y_\target) = - \sum_{i \in \setclasses} y_\text{predict}^{(i)} \log y_\target^{(i)}
\end{equation}

Gradient descent?


\section{Autoencoder}

A generative model is a function $g$ that can generate points in $\inputspace$ that look similar to points in a dataset; finding such a model is equivalent to finding the distribution of points in the dataset, denoted by $P(x)$.

A latent variable generative model is a mapping $\dec: \latentspace \to \inputspace$ that allows modeling the data distribution $P(x)$ by assuming a given $x \in \inputspace$ has been generated by drawing a latent variable $z$ from a known distribution with support in $\latentspace$ and then setting $x = \dec(z)$.
Intuitively, drawing from a tutorial by \citeauthor{doerschTutorial2021} \cite{doerschTutorial2021}, consider the problem of generating realistic handwritten digits.
Here, one could sample a digit from 0 to 9 randomly (following, say, a uniform distribution) and then relying on $\dec$ to produce an image that looks like that digit, in handwritten form. In this case, the digit would be the latent variable $z$.

A subclass of generative models is that of autoencoders.
An autoencoder is a pair of functions, commonly referred to as the ``encoder'' and ``decoder'' and denoted by $(\enc: \inputspace \to \latentspace, \dec: \latentspace \to \inputspace)$ implicitly defining a latent space $\latentspace$, with the constraint that the reconstruction error $\norm{x - \dec(\enc(x))}$ is as small as possible.
In our work we make use of autoencoder models that are based on deep neural networks and are thus differentiable.
In addition, for our purposes it is not sufficient that the reconstruction error be minimized: we require continuity of the decoder, so that mapping a line in $\latentspace$ through $\dec$ results in a continuous path in $\inputspace$.

\subsection{Variational autoencoder}

A variational autoencoder (or VAE for short) is an autoencoder model in which we learn a distribution $Q(z | x)$ that corresponds to the likelihood of a $z$ giving rise to a particular $x$.
This distribution is assumed to be a standard Gaussian and we enforce this by minimizing the KL-divergence $\kl(\normal(\mu(x), \Sigma(x)) \| \normal(0, I))$ where $\mu(x)$ and $\Sigma(x)$ are learnable networks that take $x$ as input and output respectively the mean and the covariance matrix of the Gaussian.
The encoder $\enc$ is then given by $\enc(x) = \mu(x)$.

For a more detailed introduction to VAEs we highly recommend the tutorial by \citeauthor{doerschTutorial2021} \cite{doerschTutorial2021}.

\subsection{Flow-based models}

Another type of generative model is the normalizing flow (or NF for short), in which we assume again a latent variable $z$ that is typically standard Gaussian-distributed and that leads to an $x$ via a transformation $g$ (our $\dec$ function).
Usually, this $g$ is decomposed into several small perturbations $g_i$ so that $x = g(z) = (g_K \after g_{K-1} \after \ldots \after g_1) (z)$.
Importantly, each $g_i$ is \emph{invertible}, hence $g$ itself is invertible and one can easily express and therefore maximize the log-likelihood $\log P(x)$ using the change-of-variable formula:
%
\begin{align*}
    P(x)               & = P(z) \abs{ \det \diff{g}{z} }       \\
                       & = P(z) \abs{ \det \left(
        \diff{g_K(z_{K-1})}{z_{K-1}}
        \diff{g_{K-1}(z_{K-2})}{z_{K-2}}
        \ldots
        \diff{g_1(z)}{z}
    \right)}                                                   \\
                       & = P(z) \abs{
        \det \left(
        \diff{g_K(z_{K-1})}{z_{K-1}}
        \right)
        \det \left(
        \diff{g_{K-1}(z_{K-2})}{z_{K-2}}
        \right)
        \ldots
        \det \left(
        \diff{g_1(z)}{z}
        \right)
    }                                                          \\
                       & = P(z) \abs{
        \det \left(
        \diff{g_K(z_{K-1})}{z_{K-1}}
        \right)
    }
    \abs{
        \det \left(
        \diff{g_{K-1}(z_{K-2})}{z_{K-2}}
        \right)
    }
    \ldots
    \abs{
        \det \left(
        \diff{g_1(z)}{z}
        \right)
    }                                                          \\
    \implies \log P(x) & = \log P(z) + \sum_{i=1}^K \log \abs{
        \det \diff{g_i(z_{i-1})}{z_{i-1}}
    }\quad \text{(defining $z_0 = z$)}
\end{align*}
where $\det$ denotes the determinant and $\diff{g}{z}$ the Jacobian matrix of $g$.

Unlike VAEs, the transformation $g$ is guaranteed to be invertible, but this requires that the dimensionality of $\latentspace$ be at least $\inputdim$.

\subsubsection{Nonlinear independent components estimation}

This implementation of a NF, hereafter referred to as NICE, allows crafting flows $g_i$ that are invertible, even if they contain highly nonlinear components.

This is achieved by separating $z_i$ into two parts: one left unmodified and the other put through some nonlinear layer with learnable parameters.
By construction, the transformation is invertible regardless of the nonlinearity: in \autoref{eq:coupling_layer} we show the relation between layer $i$ and layer $i+1$, with the linearity in this layer denoted by $h_{i+1}$. $z_i$ is divided into parts $A$ and $B$, and it is possible to get $z_i$ back completely from $z_{i+1}$.

\begin{equation}
    \label{eq:coupling_layer}
    \begin{cases}
        z_{i+1}^A = z_i^A \\
        z_{i+1}^B = h_{i+1}(z_i^A) + z_i^B
    \end{cases}
    \Leftrightarrow
    \begin{cases}
        z_i^A = z_{i+1}^A \\
        z_i^B = -h_{i+1}(z_{i+1}^A) + z_{i+1}^B
    \end{cases}
\end{equation}

The NICE architecture was first presented in \cite{dinhNICE2015}.

\end{document}